[
["index.html", "ML Case Studies Preface", " ML Case Studies 2020-06-03 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References "],
["reproducibility.html", "Chapter 1 Reproducibility of scientific papers", " Chapter 1 Reproducibility of scientific papers This chapter contains a wide range of studies on the reproducibility of scientific articles. Each subsection is a self-contained paper answering a different research problem. Please, note that each subsection contains a work of different authors and therefore subsections may differ at some points, for example, definitions of reproducibility used in particular studies. "],
["title-of-the-article.html", "1.1 Title of the article", " 1.1 Title of the article Authors: Author 1, Author 2, Author 3 (University) 1.1.1 Abstract 1.1.2 Introduction and Motivation 1.1.3 Related Work 1.1.4 Methodology 1.1.5 Results 1.1.6 Summary and conclusions "],
["how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html", "1.2 How to measure reproducibility? Classification of problems with reproducing scientific papers", " 1.2 How to measure reproducibility? Classification of problems with reproducing scientific papers Authors: Paweł Koźmiński, Anna Urbala, Wojciech Szczypek (Warsaw University of Technology) 1.2.1 Abstract After quite short time the computational aspects of scientific papers become obsolete or inexecutable (e.g. codes are not valid with current environment, resources are inaccessible, some classes require additional parameters). The extent of such a loss of timeliness of a paper may vary a lot. We develop a scale suitable for comparing reproducibility of various papers. This scale is based on enumerating and subjective evaluation of the impact of irreproducibilities on the reception of the paper. 1.2.2 Introduction The idea of reproducibility of scientific researches is crucial especially in the area of data science. It has become more important along with the development of methods and algorithms used in machine learning as they are more and more complex and complicated. This issue concerns users of all types: students, scientists, developers. Moreover, attaching code used in a paper, helps readers to focus on the real content rather than sophisticated explanations and descriptions included in the article. It is also valuable because the users can use the code as examples of using the package. However problem of the reproducibility is much more complex, because there is no explicit way of measuring it. It means that most of its definitions divide articles into 2 groups - reproducible and irreproducible. Thus, finding an appropriate reproducibility metrics, which would have wider set of values would result in changing the way reproducability is perceived. As a result such a metric would provide much more information for a person who would be interested in reproducing an article. 1.2.2.1 Definition Reproducibility as a problem has been addressed by scientists of various fields of studies. The exact definition also differs among areas of studies. For instance, Patrick Vandewall in 2009 suggested a definition of a reproducible research work: “A research work is called reproducible if all information relevant to the work, including, but not limited to, text, data and code, is made available, such that an independent researcher can reproduce the results” (Vandewalle, Kovacevic, and Vetterli 2009). On the other hand, Association for Computing Machinery (Computing Machinery 2018) divides the problem into three tasks as follows: Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation. Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts. Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently. For the needs of this chapter we will use the Vandewalle’s definition and treat papers as fully reproducible only when they meet the conditions listed there. 1.2.3 Related Work Reproducibility is a hot topic. “Open Science in Software Engineering” (Fernández et al. 2019) describes the essence of open source, open data, open access and other openness. The article mentions that ability to reproduce work is important for the value of research. Open Science has many positive effects: increases access and citation counts, supports cooperation through open repositories. “Reproducibility Guide” (“Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing” 2014) contains a lot of informations and tips on how to make research easier to reproduce. The guide also contains the list of tools that can make our research more reproducible (for example version control and automation. And the most important for us: it includes the checklist of questions that help verify the ability to reproduce. Edward Raff emphasizes the word independent in his article (Raff 2020). Independent reproducibility means that we can obtain similar results independently of the author and his code. These articles highlight various aspects of reproducibility. We want to verify how the authors care about reproducibility, what are their biggest reproduction issues and what type of problems can we encounter reproducing articles. 1.2.4 Methodology We considered plenty of papers from many issues of The R Journal - one of the most popular magazines concerning scientific researches, including new R packages. The journal stands out from the magazines because the researches usually upload supplementary material with their articles so it is very easy to check if the code can be reproduced in right way. The articles we checked during our research were published in various years - the newest comes from December 2019 while the oldest is from 2009. We have to admit that the majority of articles could be reproduced without any problems. For the needs of this article we mention only papers where any problems occured. As we faced the problem of measuring reproducibility we discussed many ways of grading its level. One of the ideas was to create a unified measure of value that would calculate the ratio of functions that managed to execute. We quickly noticed that this approach is not appropriate as sometimes it is not fair to dock the mark by the same value in various examples. For instance we could meet a minor problem that interefered with executing an additional feature and, on the other hand, a vast problem that was a reason that we could not produce an important plot at all. Moreover, sometimes the success of executing the function, was not only defined by completing the work without errors but by the output’s quality. Sometimes the plots were produced without any number values what made them absolutely useless or without one minor annotation which still allowed to make similar conclusions as authors. These were the reasons why we did not decide to create a simple numeric measure of reproducibility, which probably would be very convenient for data scientists, especially statisticians. When we were checking the articles in terms of reproducibility we noticed that the problems we are facing can be group into a few categories of similar ones. It was on impulse to propose six major categories that can be faced during try of reproducing the results presented in a scientific paper: No access to external resources Some parts of code require access to external resources, for example third-party API or data downloaded from web. If the data was removed from the website, we may have a problem (or it can be impossible!) reproducing the results. No compatibility with current versions of used packages Some packages are deprecated or only available in the older version of R. It can cause problems and is unacceptable. Code vulnerable to user settings (especially graphic issues) The output often depends on the environment settings. For example the scale of the graphics can make it illegible and useless. There were cases that the code attached to article produced completely different figure than the presented one. Additional configuration required Some packages require a non-standard installation. To use some features it can be required to install system packages. Sometimes it is also required to take additional steps (configure access to API e.t.c.). Randomness problems Some functionalities are based on randomness. Sometimes changing the seed may change the results and make it difficult to draw correct conslusions. No access to source codes Some results shown in an article could not be reproduced because the codes had not been attached to or included in the article. We developed a 6-point scale (0 - completely irreproducible, 5 - fully reproducible) evaluating in what degree these problems belong to each category. Points are assigned subjectively depending on our feeling of the severity of the problem. When a category did not appear in the article, it was signed as N/A - not applicable. To minimilize the affect of the personal feels, every article was checked independently by at least two persons. 1.2.4.1 Mark examples Most of the articles were reproducible to some extent. None of them were fully irreproducible. However there were few examples, where inability of compiling the first chunks of code resulted in very low marks for the article and thus giving it up with no further research being carried through. Perfect example of such behaviour can be found in an article “MCMC for Generalized Linear Mixed Models with glmmBUGS”(Brown and Zhou 2010), where all of the following code chunks depended on first ones, which couldn’t be compiled. The reason was that the function, which was resposible for making crucial calculations couldn’t find a registry address. It ended up with displaying both the error and warning message. Second thing which led to lowering the mark was difficulty with code availibility. There were articles, for instance “RealVAMS: An R Package for Fitting a Multivariate Value-added Model (VAM)” (Broatch, Green, and Karl 2018), where there were no source codes for all of the figures, which were used in article. Moreover the figures were the main part of the article, thus we decided to lower the mark for access to the source code. Fortunately our team of scientists managed to reproduce the results, despite lack of source code. This article was also an example of having attached obsolete data. It resulted in poor similarity of graphs and plots between the figures we made ourselves and those, which were used in article. Majority of articles were given very satsfying marks, beacuse there were only a few things we could complain about. Fortunately they didin’t have such an impact on reproducibility itself, but rather were annoying for someone who wanted to achieve the same results. The perfect example of such an article is “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution” (Wilhelm and Manjunath 2010). The code had to be manually copied from the article and then reformatted before pasting in into the R console. It’s not a major obstacle, but it may lead to some syntax mistakes and enlengthen the time needed to reproduce the results. 1.2.5 Results During the research, our team of scientists examined 16 scientific articles published in The R Journal in terms of reproducibility. As stated before, we decided to divide a mark into six categories and check the level of correctness with the results described in the paper. Every category was graded in six-point scale provided always that a category might not apply in a paper. However, we did not measure the effectivity or functionality of the code as it was not in the scope of this research. To avoid the effect of subjectivity, all articles were graded by at least two of us. Later we calculated the average of the marks. When any category was marked as “not applicable” by at least one marker, it was not taken into consideration in the final summary. The list of articles checked is here. The summary of our marks is presented in the boxplot. Articles’ marks distribution As we can see in the plot, packages dependent on external resources are in a minority. Despite that, when we examined one, there was often a problem with dependencies. It could be caused by many reasons, e.g. external data sources or other packages. On the other hand, availability of the source code and graphical aspects of articles turned out to be the most reproducible categories. Resistance to randomness was one of the most interesting categories in our opinion and it turned out to be a category with high variation of grades. Many authors coped with the problem by setting random seeds but sometimes the differences were unavoidable. 1.2.6 Summary and conclusions References "],
["aging-articles-how-time-affects-reproducibility-of-scientific-papers.html", "1.3 Aging articles. How time affects reproducibility of scientific papers?", " 1.3 Aging articles. How time affects reproducibility of scientific papers? Authors: Paweł Morgen, Piotr Sieńko, Konrad Welkier (Warsaw University of Technology) 1.3.1 Abstract Reproduction of a code presented in scientific papers tend to be a laborious yet important process since it enables readers a better understanding of the tools proposed by the authors. While recreating an article various difficulties are faced what can result in calling the paper irreproducible. Some reasons why such situations occur stem from the year when the article was published (for example usage of no more supported packages). The purpose of the following paper is to prove whether this is a general trend which means answering the question: is the year when the article was published related to the reproducibility of the paper. To do so a package CodeExtractorR was created that enables extracting code from PDF files. By using this tool a significant number of articles could be analyzed and therefore results received enabled us to give an objective answer to the stated question. 1.3.2 Introduction Every article published in a scientific journal is aimed at improving our knowledge in a certain field. To prove their theories, authors should provide papers with detailed, working examples and extensive supplementary materials to reproduce results. Unfortunately, these conditions are not always fulfilled. In such a case, other researchers are not able to verify and accept the solutions presented by the author. Moreover, the article is not only useless for the scientific community but also for business recipients. Over the years, several different definitions of reproducibility have been proposed. According to Gentleman and Temple Lang (2007), reproducible research are papers with accompanying software tools that allow the reader to directly reproduce methods that are presented in the research paper. Other authors suggest that scientific paper is reproducible only if text, data and code are made available and allow an independent researcher to recreate the results (Vandewalle, Kovacevic, and Vetterli 2009). Second definition emphasizes the importance of accessibility to data used in researches, therefore it seems to be more suitable and complete interpretation of reproducibility. In addition, in this article, we used scale based on the spectrum of reproducibility, proposed by Peng (2011). In his work, he also mentioned reproducibility as a minimal requirement for assessing the scientific value of the paper. In the past few years, computing has become an essential part of scientific workflow. Some best practices for writing and publishing reproducible scientific article were presented by Stodden et al. (2013). Furthermore, she made a brief overview of existing tools and software that facilitate this task. Similar issue was closely described by Kitzes, Turek, and Deniz (2017). Tools created solely for reproducibility in R were proposed by Marwick (n.d.) in package rrtools. Although many articles focus on software or framework solutions for reproducibility problems, analysis of scientific papers reproducibility in the context of release date has, to the best of the authors’ knowledge, not been described before. The intention of such research is to find correlations between age of article and its reproducibility. Authors believe that finding these dependencies will allow to calculate the estimated life span of data science article. Furthermore, as replicability helps with applying proposed methods and tools, its approximated level might be helpful in estimating usefulness of every scientific article. 1.3.3 Methodology The first issue that should be touched upon, while considering the methodology behind preparing this article, is the scale used to assess the reproducibility of the papers. In the Introduction it was already mentioned how the scale was created but a more detailed description is required. The authors decided that the scale should consist of 4 levels (from 0 up to 3): The 0 grade was given in case when no chunk of code gave the anticipated results and no figure was reproduced successfully (in practice such situation occurred mainly when the package described in the article was no more available). The 1 grade means that at least one example gave the results that the authors waited for, while it also includes situations when about half of the code in the chunks behaved as expected. The 2 grade was awarded to the articles that were reproducible “in the majority” what also means that they were not reproducible in 100%. The 3 grade was received by the articles that were fully reproducible and no problems were encountered in the process. Such a result was highly anticipated by the authors but the criteria for this grade were rather strict. The second issue that also played a vital role in the authors’ work was the scope of the analysis. It was decided that in order to maintain “other thing equal” according to a well-known Latin phrase “ceteris paribus” only one online journal – The R Journal – was taken into account. Being equipped with a tool for faster reproduction of articles – the CodeExtractoR package – the authors agreed to examine about 20 articles that were published across a few years. Such a great number of papers meant that the approach taken could be described as holistic. It is also worth mentioning that usually 30 articles from each year were analyzed (at least whenever it was possible). Finally, it should be noted that in the case of the date when they were published the examined articles range from 2009 up to 2019. The third and final issue that should be considered in this part of the article focuses on the measures undertaken by the authors in order to tackle the problem of biased assessment. Although the scale that was proposed was not totally dependent on the person who was using it, it still left someplace for personal liking and disliking of the paper. As a way of marginalization of this trend the authors have taken part in many conversations when the facts that led to specific grading of the articles were shared. This enabled awarding the grades even more fairly. However, the final measure was much more simple and it was believed to be much more effective as well when compared to the previous one. The articles for each year were divided into 3 groups and assigned to one author each. Thus each author has examined the papers from the whole range of release dates that were taken into account. 1.3.4 Results Specific results are presented in Table 1.1, which shows the number of examined articles from 2009 up to 2019, grouped by received grade. The column “Grade” represents the 0 - III scale of reproducibility. The rest of the columns shows a number of papers that achieved a particular grade in each year. Grade 09’ 10’ 11’ 12’ 13’ 14’ 15’ 16’ 17’ 18’ 19’ 0 3 2 6 4 6 8 5 9 5 9 9 I 4 2 0 2 3 6 6 10 3 2 2 II 3 7 8 4 13 6 10 7 14 6 6 III 6 7 6 5 8 10 9 4 8 13 13 SUM 16 18 20 15 30 30 30 30 30 30 30 To better illustrate obtained data, in Figure 1.1 we have split the results into two groups - the articles which have 0 or I class labeled as “Non-reproducible” and articles with grade II and III as “Reproducible”. It is important to remember that from 2009 to 2012, the overall number of papers oscillated around 18 per year. After 2013, the number of researched articles was constant. FIGURE 1.1: Number of papers by publication year Figure 1.2 shows the results in the original 4-level scale. Although, the number of papers varies throughout the years in every reproducibility class, it is observable that intermediate ones - I and II, are less common in the oldest and newest papers. In addition, results in 2018 and 2019 are identical. FIGURE 1.2: Number of papers by class and publication year After calculating the percentage of each class in a specific year ( Figure 1.3, Figure 1.4), it is observed, that in the two oldest examined years - 2009 and 2010 - a ratio of completely unreproducible papers (with 0 or I class ) is surprisingly low. Furthermore, papers with III class of reproducibility are nearly 40% of all articles in these years. FIGURE 1.3: Ratio of each class throughout years Except for 2019, 2018 and 2016, percentage of fully reproducible papers (III class) is stable. In the newest articles, this percentage is slightly higher. Year 2016 is the only one, where unreproducible papers were in the majority. Only in 3 cases, percentage of reproducible articles dropped below 60%. FIGURE 1.4: Summarized results throughout years 1.3.5 Conclusions 1.3.6 Summary References "],
["ways-to-reproduce-articles-in-terms-of-release-date-and-magazine.html", "1.4 Ways to reproduce articles in terms of release date and magazine", " 1.4 Ways to reproduce articles in terms of release date and magazine Authors: Mikołaj Malec, Maciej Paczóski, Bartosz Rożek 1.4.1 Abstract 1.4.2 Introduction and Motivation Reproducibility is a topic which is quite diminished in today’s science world. Scientific articles should be current as long as possible. Their results should be achievable by reader and be the same. Thanks to that science and business world can take advantage of them.The more article is difficult to reproduce, the chance of using knowledge coming from it is smaller. Many researchers tried to define or give principles for reproducibility. There is article published in 2016: “What does research reproducibility mean?” (Goodman, Fanelli, and Ioannidis 2016b) which tried to warn about reproducibility crisis. Article in 2017: “Computational reproducibility in archaeological research: basic principles and a case study of their implementation” (Marwick 2016), compered computational reproducibility to archaeological research and give guidelines for researches to use reproducibility in computing research. But these are just two of many articles about reproducibility. Some articles are about tools and techniques for computational reproducibility (Piccolo and Frampton 2016). They encourage researchers to compute data using environments like Jupiter (Thomas et al. 2016) or R markdown (Marwick, Boettiger, and Mullen 2017). Thanks to that readers can reproduce finding on their own. What’s new about our approach to the subject of reproducibility is focusing on how can release date and magazine affect the amount of work needed to fully reproduce code or is it even possible. A comprehensive comparison of scientific magazines in terms of reproducibility is yet to be created and this article is our best effort to make it happen. Mikołaj Malec 1.4.3 Related Work 1.4.4 Methodology 1.4.5 Results 1.4.6 Summary and conclusions References "],
["reproducibility-of-outdated-articles-about-up-to-date-r-packages.html", "1.5 Reproducibility of outdated articles about up-to-date R packages", " 1.5 Reproducibility of outdated articles about up-to-date R packages Authors: Zuzanna Mróz, Aleksander Podsiad, Michał Wdowski (Warsaw University of Technology) 1.5.1 Abstract The inability to reproduce scientific articles may be due to the passage of time. Factors such as changes in data or software updates - whether author-dependent or not - may make it difficult to reproduce the results after a long time. It would seem obvious that old scientific articles about R-packages are usually difficult to reproduce. But what if this package is still supported? In what way the continuous support changes the possibility of reproducing these articles? In this article we will look at examples of still updated R-packages from 2010 or older in order to analyse the type and degree of changes. 1.5.2 Introduction and Motivation The problem of the inability to reproduce the results of research presented in a scientific article may result from a number of reasons - at each stage of design, implementation, analysis and description of research results we must remember the problem of reproducibility - without sufficient attention paid to it, there is no chance to ensure the possibility of reproducing the results obtained by one team at a later time and by other people who often do not have full knowledge of the scope presented in the article. Reproducibility is a problem in both business and science. Science, because it allows credibility of research results (McNutt 2014). Business, because we care about the correct operation of technology in any environment (Anda, Sjøberg, and Mockus 2009). As cited from “What does research reproducibility mean?” (Goodman, Fanelli, and Ioannidis 2016b); “Although the importance of multiple studies corroborating a given result is acknowledged in virtually all of the sciences, the modern use of “reproducible research” was originally applied not to corroboration, but to transparency, with application in the computational sciences. Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables. This concept has been carried forward into many data-intensive domains, including epidemiology, computational biology, economics, and clinical trials. According to a U.S. National Science Foundation (NSF) subcommittee on replicability in science, “reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative.” 1.5.3 Related Work Other notable articles about reproducibility include “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” (Anda, Sjøberg, and Mockus 2009), “Reproducible Research in Computational Science” (Peng 2011) and “A statistical definition for reproducibility and replicability” (Patil, Peng, and Leek 2016). “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” focuses on the variability and reproducibility of the outcome of complete software development projects that were carried out by professional developers. “Reproducible Research in Computational Science” is about limitations in our ability to evaluate published findings and how reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible. “A statistical definition for reproducibility and replicability” provides formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press. In our article we focus on the reproduction of old scientific articles on R and packages, which are still being developed. We want to explore how the passage of time affects the ability to reproduce results using the currently available updated tools. We are therefore testing backward compatibility for different packages and checking what affects the reproducibility of the code. We were unable to find scientific articles on this exact issue. There are articles that give ways to measure reproducibility, as well as articles about packages that help with reproduction. But there are yet no articles that summarize the set of packages in terms of their reproducibility. 1.5.4 Methodology We have checked 13 articles with 16 R packages from at least 10 years ago to ensure that the code chunks match these categories: ade4: Implementing the Duality Diagram for Ecologists (Dray and Dufour 2007) AdMit: Adaptive Mixtures of Student-t Distributions (Ardia, Hoogerheide, and Dijk 2009) asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples (Coeurjolly et al. 2009) bio.infer: Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition (Yuan 2007) deSolve, bvpSolve, ReacTran and RootSolve: R packages introducing methods of solving differential equations in R (Soetaert, Petzoldt, and Setzer 2010) EMD: Empirical Mode Decomposition and Hilbert Spectrum (Kim and Oh 2009) mvtnorm: New Numerical Algorithm for Multivariate Normal Probabilities (Mi, Miwa, and Hothorn 2009) neuralnet: Training of Neural Networks (Günther and Fritsch 2010) party: A New, Conditional Variable-Importance Measure for Random Forests Available in the party Package (Strobl, Hothorn, and Zeileis 2009) pls: principal Component and Partial Least Squares Regression in R (Mevik and Wehrens 2007) PMML: An Open Standard for Sharing Models (Guazzelli et al. 2009) tmvtnorm: A Package for the TruncatedMultivariate Normal Distribution (Wilhelm and Manjunath 2010) untb: an R Package For Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity (Hankin 2007) All articles come from the R Journal and Journal of Statistical Software. In our research on the subject we have decided to divide the code from the articles into chunks, according to the principle that each chunk has its own output, to which we give an evaluation according to the criteria we have set. In the process of testing and reproducing various articles, we have identified five categories, and marked them as follows: NO REP (no reproducibility, either due to changes through time or problems with the article that had already been there, regardless of differences in R across years), HAD TO CHANGE STH (when we had to modify the code to produce correct results that will work in our current R version and can be neatly displayed in a document generated by markdown), MOSTLY REP (when the results were not ideally identical to the original, but in our opinion the chunks were working according to their purpose in the context of their article), HAD TO CHANGE &amp; STILL SOMEWHAT DIFFERENT (the code had to be changed and the results were not perfect, but they were correct in the terms of the aforementioned category MOSTLY REP, but we could consider them as satisfactory), FULLY REP (no reproductive problems - the results were identical to original results shown in the article). These criteria can be considered not subjective, but setting such boundaries does not cause confusion in categorisation, thus we decided to use them in order to research and describe the introduced number of articles about R packages from at least 10 years ago. In some of the articles we found specific types of problems: There was no access to data or objects referred to in later calculations, The results were similar to the original, but the differences were most often due to the random generation of objects. This error was usually reduced later, when the package created some kind of data summary - then the result had a very small relative error with the original result. The names of individual variables or some of their attributes changed (e.g. column names in the data frame). When data or objects were unaccessible and there could not be found any alternative or history of the dataset, we classified this chunk as NO REP. What is more, in most cases chunks dependent on such objects automatically became NO REP too. However, if the code was only partially dependent on the lost dataset, it was classified as MOSTLY REP. When we stumbled upon problems with randomly generated objects, where the values were obviously different, but after aggregating the data summaries were close to original, such chunks were marked as MOSTLY REP. As we were reproducing various articles, this has become quite significant problem that a large number of publications were struggling with. If data could be somehow fixed - for example by changing column names, it was given HAD TO CHANGE STH mark, and the dependent chunks’ marks were not influenced by this change, which theoretically was allowing them to be marked even as FULLY REP. 1.5.5 Results Here can be seen the in-depth report. Below are the summarised results of our research. As it can be seen, the vast majority of chunks are fully reproducible. Even if the chunk is not executed identically to the original one, in most cases it differs only slightly, and the package itself serves its purpose. 88.3% (121/137) of the chunks are executed perfectly or correctly (within our subjective category of being acceptably incorrect), while 93.4% (128/137) of the chunks are working well enough not to throw errors. In practice, only 6.6% (9/137) of chunks were completely irreproducible, which would seem surprising for more-than-a-decade-old articles. However, given that we have focused particularly on packages that are still being developed, this is quite a feasible result. This can be seen quite clearly by the percentage of the chunks that required minor changes or slightly differed from the results shown in the article - there were 33.6% (46/137), which is clearly a result of the updates or changes that occured in the ever evolving R environment. Of course during our research we stumbled upon numerous packages that have not been updated since years or that have even been deleted from CRAN repository, so they were not within our field of interest. Nonetheless, we would like to emphasize that the results should not suggest that one-decade-old articles are reproducible. 1.5.6 Summary and conclusions To sum up, in most cases the packages we examined performed their tasks correctly. The packages themselves have of course changed, but its idea remained the same. Usually new features or improvements were added, but the idea behind the package was the same as it used to be. As a result, most of the packages still managed to cope with the problems of the old ones, in reproduction usually suffering from missing external data or unavoidable changes in the R language itself. All in all, almost in all cases the package does the job in spirit, differing from its old good ways only in vague confusion caused by neverending winds of change. It can therefore be concluded that most packages that we’ve checked are fully backward compatible, which is good programming practice. In order to increase the reproducibility of articles, this should definitely be taken care of. Additionally, authors should include supplements to their articles, that always help you understand and copy the code. References "],
["correlation-between-reproducibility-of-components-of-research-papers-and-their-purpose.html", "1.6 Correlation between reproducibility of components of research papers and their purpose", " 1.6 Correlation between reproducibility of components of research papers and their purpose Authors: Przemysław Chojecki, Kacper Staroń, Jakub Szypuła (Warsaw University of Technology) 1.6.1 Abstract 1.6.2 Introduction and Motivation It is common knowledge that reproducibility is a way for science to evolve. It is the heart of the scientific method to revisit pre-existing measurements and to try to reproduce its results. However, the term „reproducibility” itself, as well it is crucial to the scientific methodology, it can be also universal at the expense of unambiguousness and usability. For the purpose of this paper we will have recourse to the definition introduced by ACM: Reproducibility - The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same resultusing artefacts which they develop completely independently. This particular definition ilustrates perfectly how in the course of establishing the meaning of term „Reproducibility”, the level of importance of auxiliary measurements and settings of the experiment to the overall results is omitted. It is notably significant misconception especially in the experiments from the field of computional science, when reproducing or even maintaining precise operating conditions is usually impossible. In the following chapters we will attempt to perform an analysis of reproducibility of the papers submitted to the RJournal, regarding especially presumed objectives of enclosed auxilliary computations and artifacts (i. e. code chunks) in overarching structure of a given paper. 1.6.3 Related Work Although there are many research papers related to this article, the following three could be perceived as a “basis” for our study. 1. Daniel Mendez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold provides a definition of reproducibility this article uses, and distinguishes it from replicability(Fern’andez et al. 2019). 2. Steven N. Goodman, Daniele Fanelli and John P. A. Ioannidis defines multiple interpretations of reproducibility. It further divides and classifies reproducibility, and provides a basis on how one can do it(Goodman, Fanelli, and Ioannidis 2016a). In their search the authors have not encountered other research papers that study the aspect of reproducibility this article focuses on. If said papers do not actually exist, then this article could provide insights on previously unexamined aspects of reproducibility. 1.6.4 Methodology 1.6.4.1 General approach The methodology presented in the following section is a direct consequence of how we approach scientific article as an experience devised by an author for the reader. Our focus is to determine, how author alter this experience by using code chunks instead of plain text. In other words, we ask a question “Why have the authors used the R code?” Assumption that execution of enclosed code is an integral part of said experience and by extension code chunks supposed to be reproducible for reader to percept the article as intended seems to be reasonable. However it may not be correct in every case. Let us consider a situation, where generated output is essential to the thesis stated in the article. If the code is irreproducible, the reader cannot believe the authors. It devastates their credibility. But what if a goal of the code was to illustrate general tendency in data and output is reproducible only to some degree? Then it may still fulfill its purpose in the article and the lack of full reproducibility does not intefere with experience for a reader. Following this thought process inevitably leads to new questions, f.e. is it possible for executable code to serve its purpose in article while being completely unreproducible? To explore this topic we decided to focus on objectives of code in scientific papers. We have decided that the most accurate and reliable way of finding the purposes of code chunks in scientific articles is by examples. That is why we have analyzed over \\(30\\) papers from The R Journal [https://journal.r-project.org]. We have gathered the code chunks into groups and considered a three degree of purpose: the whole article, the group of chunks, and the single chunk of code. We have prepared a list of possible purposes for every level and assign them to our examples. The whole list of purposes is explained in the next chapter. Then we have produced our measure of reproducibility, which is also detailed later. 1.6.4.2 Objectives Since this article is centred around objectives, our understanding of them is of utmost importance. That is why we divided them into three categories, further divided into classes. We described them in detail in the sections below. To limit our individual biases in assessing what the intended objective is, we referred to relevant paragraphs in the original research paper. It has to be noted, that an object (let it be an article, a group of chunks or a chunk) can have more than one objective. 1.6.4.2.1 Main Objectives Both code chunks by themselves and performed computations corresponding with them can provide wide variety of information. However we can identify and describe reasons why the programming language is present in general in a given paper. All code chunks serve together as a vital element supporting narration of the article and its objective usually can be identified with main objective of narration in article as a whole. We systematized main objectives and grouped them into the following general objectives: package overview - presenting general structure of specific package, providing example of aplications implemented functions and discussing its performance object construction - presenting process of constructing and developing virtual object introduction to subject - using performing code as a complement to educational resource concerning given topic method implementation - presenting in-depth process of developing solution and explaining it addressing an issue - presenting solution to specific scientific/computational problem error identification - recognising and presenting error in existing package, possibly submit alternative solution covering mentioned error 1.6.4.2.2 Intermediate Objectives Since code chunks in research papers seldom appear on their own, but rather are part of a larger group of chunks serving a certain purpose. For instance, let there be three chunks, named A, B, and C. Let A load data from an external source, B modifies the data and extract a subset of it, and let C generate a plot, based on the data obtained from the two previous chunks. While each chunk has its own distinct objective, together they have at least common one - in this example this is generating a plot. Plot generated by A, B and C can be used to compare between performance of various functions. These chunk group’s objectives we define as intermediate objectives. We systematized intermediate objectives and grouped them into the following general objectives: package usage - examples on how does an R package operate, how one can use functions provided by the package, in what manner output is generated etc. practical use - underscoring of the practical usage of code used in code chunks in that group. method comparison - comparison between functions and/or methods. For example, a microbenchmark between base R functions and functions from a presented package. generating output - generating an output, for example plots, .csv files, images etc. presenting specification - presentation on what package specification looks like. data preparation - preparation of data that may be used later in the paper. This includes both loading the data and modifying it. occurrence demonstration - demonstration of an occurrence described earlier in the article. introduction to analysis - introduction to analysing a certain topic and data related to it. possible error - description of a possible error one can encounter and how one can solve it. 1.6.4.2.3 Chunk Objectives Each chunk has a role - it serves one or more purposes, which we define as chunk objectives. We systematized chunk objectives and grouped them into the following general objectives: aesthethic example - an example showcasing how output generated by the code chunk looks like. functional example - an example of how functions showcased in the chunk work. instruction - an instruction on how one achieves desired effect using R code. instruction (package) - same as above, but using functions from the package introduced in the article containing the chunk. data preparation - preparation of data for the following chunks. data exploration - merging, subsetting, summarisation of data and other types of data manipulation used in order to explore data. foreign error - turning attention to an error in work done by other author(s). solving a problem - description of how one solves a given problem using R code. data modification - modifying data in order to achieve desired effect. presentation of results - presenting result of computation within the article. This can be done by specific summarising functions (e.g. summarise) or simply printing base R vectors. plot - plotting graphs in the article. generating files - generation of files, this includes graphical, text and other files. results processing - processing of results in order to improve their aesthethic value or to make them more readable. erroneous action - presenting code that does not run properly as an example of an action should be avoided. uncallable code - code that, in principle, is impossible to run. This includes pseudocode. comparison to foreign work - comparation of authors’ work (functions, methods etc.) with work of others, that achieves the same effect. This includes benchmark performance comparisons. empirical proof - validation of what is mathematically described in earlier sections. 1.6.4.3 Reproducibility The sole purpose of this paper is to explore interactions between purposes of code chunks usage and reproducibility aberrations. That requires a system of classification of reproducibility. We provide simple categorization of forms of reproducibility into the 6 types. This classification system shall serve as a tool for initial phase of our analysis, thus it is not directly involving purpose of discussed code at this stage. perfect reproducibility - code perform flawlessly and after initial configuration precise output is recreated margin of error - after initial configuration code provides output matching expectations within acceptable margin of error (f.e. difference in rounded decimals, default parameters of generated graphics) editorial correction - code requires minor corrections to be executable and viable due to editorial error or changes in naming conventions environment setup - code to execute properly requires major and time-consuming setup and environment changes or may be not able to provide expected results at all unreproducible - code undoubtedly cannot be reproduced (f.e. due to unavailable data, unavailable package, unsupported fatal error) -1. missing point of reference - article does not provide (or vaguely provides) expected performence and determining reproducilibity is impossible 1.6.4.4 Tables description For analysis purposes, we have put our work into tables. The one can see the small part of them here: 1.6.4.4.1 1.Table of articles Every row represents one article. Every article has a column of an individual number, a sum of lengths of chunks of code, and information about purposes of articles. 1.6.4.4.2 2.Table of groups Every row represents one group of chunks. Every group has a column of an individual number, a sum of lengths of chunks of code, and information about purposes of the group. 1.6.4.4.3 3.Table of chunks Every row represents one chunk. Every chunk has a column of an individual number, its length of code, its reproducibility, and information about purposes of code. 1.6.4.4.4 Length assessment To objectively determine a length of code we have decided to count it with such rules: * skip all empty and commented lines * skip assignments, unless it contains the execution of a function * skip executions of functions library and data * skip lines with only technical meaning, i.e. } 1.6.5 Results 1.6.6 Summary and conclusions References "],
["how-active-development-affects-reproducibility.html", "1.7 How active development affects reproducibility", " 1.7 How active development affects reproducibility Authors: Ngoc Anh Nguyen, Piotr Piątyszek, Marcin Łukaszyk (Warsaw University of Technology) 1.7.1 Abstract 1.7.2 Introduction and Motivation The key quality in measuring the outcome of researches and experiments is whether results in a paper can be attained by a different research team, using the same methods. Results presented in scientific articles may sometimes seem revolutionary, but there is very little use if it was just a single case impossible to reproduce. The closeness of agreement among repeated measurements of a variable made under the same operating conditions by different people, or over a period of time is what researches must bear in mind. Peng (2011) leading author of the commentary and an advocate for making research reproducible by others, insists reproducibility should be a minimal standard. There have been several reproducibility definitions proposed during the last decades. Gentleman and Temple Lang (2007) suggest that by reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. The second definition is according to Vandewalle, Kovacevic, and Vetterli (2009), research work is called reproducible if all information relevant to the work, including, but not limited to, text, data, and code, is made available, such that an independent researcher can reproduce the results. As said by LeVeque (2009) the idea of ‘reproducible research’ in scientific computing is to archive and make publicly available all the codes used to create a paper’s figures or tables, preferably in such a manner that readers can download the codes and run them to reproduce the results. All definitions converge into one consistent postulate - the data and code should be made available for others to view and use. The availability of all information related to research paper gives other investigators the opportunity to verify previously published findings, conduct alternative analyses of the same data, eliminate uninformed criticisms and most importantly - expedite the exchange of information among scientists. Reproducibility has great importance not only in the academic world but also it also plays a significant role in the business. The concept of technological dept is often used to describe the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer in software development. There are papers about using version control systems to provide reproducible results (Stanisic, Legrand, and Danjean 2015). The authors presented how we can manage to maintain our goal of reproducibility using Git and Org-Mode. Other researchers have created a software package that is designed to create reproducible data analysis (Fomel et al. 2013). They have created a package that contains computational modules, data processing scripts, and research papers. The package is build using the Unix principle to write programs that are simple and do well one thing. The program breaks big data analysis chains into small steps to ensure that everything is going in the right way. Some papers suggest using Docker to make sure our research can be reproduced (Hung et al. 2016). The main goal of our work is to measure the impact of the active development of packages on the reproducibility of scientific papers. Multiple authors (Rosenberg et al. 2020; Kitzes, Turek, and Deniz 2017) suggest using the version control system as a key feature in creating reproducible research. The second paper also provides evidence, that this is widely known. Git and GitHub were used in over 80% of cases. However, there are two kinds of using a version control system. An author can push software into the repository, to make it easily accessible and does not update it anymore. The second option is to keep the repository up-to-date and resolve users’ issues. We have not found any research on how these two approaches impact reproducibility. 1.7.3 Methodology Articles In our analysis, of reproducibility, we focused on articles introducing packages, that are actively developed on GitHub. Then we measure the reproducibility of an article using two versions of the package: current and the first after publication date to get the answer on the question, what if a package was never updated. In some cases, when it seems appropriate we used the last before publication. We selected 18 articles that were posted on R journal, that are on cran, are developed on GitHub, have code included to reproducibility, and doesn’t have too much impact on R environment. Measures of reproducibility We measured how many examples aren’t reproducible using these two versions. We categorized articles into 3 types of reproducibility: 1. The article is reproducible, minor differences can happen (e.g. different formating). 2. There are differences in function names, other packages that the article uses don’t work but at least half of it works. 3. Everything that doesn’t match 1 or 2. It means that the article is not reproducible. We have counted the three most common issues in each article: 1. Names - function or variable name has to be changed 2. API - way of using a function or their arguments has changed 3. Result - output differs Using these we can compare specific issues in the current and old versions of the package. Auxiliary variables To measure how a package is developed, we used several auxiliary variables from GitHub and CRAN: number of stars number of subscribers number of contributors number of issues number of open issues added and deleted lines since the publication date commits number since the publication date using Continuous Integration versions on CRAN since the publication date 1.7.4 Results Tested packages TABLE 1.1: Tested packages with measured reproducibility package old.version old.reproducibility new.reproducibility old.names.issues new.names.issues old.api.issues new.api.issues old.result.issues new.result.issues VSURF 1.0.2 2 2 4 0 2 1 3 4 MVN 3.8 1 3 0 0 0 7 0 0 mldr 0.2.51 2 2 1 1 0 0 0 0 fanplot 3.4.1 1 1 0 0 0 0 0 0 Peptides 1.0.4 2 2 0 5 0 0 0 0 cmvnorm 1.0-3 1 1 0 0 0 0 0 0 factorplot 1.1 1 1 0 0 0 0 3 3 FactoMineR 1.3 1 1 0 0 0 0 0 0 gridGraphics 0.1-5 1 3 0 0 0 0 1 3 phaseR 1.3 1 2 0 3 0 0 0 0 betategarch 3 1 1 0 0 0 0 6 6 tempdisagg 0.22 1 1 0 0 0 0 0 0 mvtnorm 0.9-7 2 2 0 0 1 1 0 0 brainR 1.2 1 1 0 0 0 0 0 0 qmethod 1.3.0 2 2 0 0 0 0 1 1 stringdist 0.7.2 1 1 0 0 1 1 1 1 rotations 1.3 3 3 0 0 0 0 3 3 ggmap 2.3 3 3 0 1 0 9 13 2 Reproducibility scale As shown in table below, most packages have same reproducibility scale in each version. Some are less reproducible in current version than in the old. old.reproducibility new.reproducibility n 1 1 8 1 2 1 1 3 2 2 2 5 3 3 2 Issues count We compared if new versions of packages have more or less issues of each type than the old ones. Only for few articles these counts differ, but this data suggests negative impact of active development on reproducibility. Correlations with auxiliary variables This heatmap shows the correlation between reproducibility scale and issue count increase (new-old) with auxiliary variables. The reproducibility scale does not seem to be correlated with any of them. But there is a strong correlation between name issues count and number of lines added and removed since the publication date. Variables associated with popularity could impact on API changes. There are correlations with results, but results should not be analyzed alone, because when API issue occurs, then we cannot check results. 1.7.5 Summary and conclusions References "],
["reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language.html", "1.8 Reproducibility differences of articles published in various journals and using R or Python language", " 1.8 Reproducibility differences of articles published in various journals and using R or Python language Authors: Bartłomiej Eljasiak, Konrad Komisarczyk, Mariusz Słapek (Warsaw University of Technology) 1.8.1 Abstract 1.8.2 Introduction and Motivation Due to the growing number of research publications and open-source solutions, the importance of repeatability and reproducibility is increasing. Although reproducibility is a cornerstone of science, a large amount of published research results cannot be reproduced (Gundersen and Kjensmo 2018). Repeatability and reproducibility are closely related to science. “Reproducibility of a method/test can be defined as the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object, or test material) but under different conditions (different observers, laboratories etc.). (…) On the other hand, repeatability denotes the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object or test material), under the same conditions.”(Slezak and Waczulikova 2011) Reproducibility is crucial since it is what an researcher can guarantee about a research. This not only ensures that the results are correct, but rather ensures transparency and gives scientists confidence in understanding exactly what was done (Eisner 2018). It allows science to progress by building on previous work. What is more, it is necessary to prevent scientific misconduct. The increasing number of cases is causing a crisis of confidence in science (Drummond 2012). In psychology the problem has already been addressed. From 2011 to 2015 over two hundred scientists cooperated to reproduce results of one hundred psychological studies (Anderson et al. 2019). In computer science (and data science) scientists notice the need for creating tools and guidelines, which help to guarantee reproducibility of solutions (Biecek and Kosinski 2017, @Stodden1240). There exist already developed solutions which are tested to be applied (Elmenreich et al. 2018). Reproducibility can focus on different aspects of the publication, including code, results of analysis and data collection methods. This work will focus mainly on the code - results produced by evaluation of different functions and chunks of code from analysed publications. In this paper we want to compare journals on the reproducibility of their articles. Moreover, we will present the reproducibility differences between R and Python - two of the most popular programming languages in data science publications.There is discussion between proponents of these two languages, which one is more convenient to use in data science. Different journals also compete between each other. There are already many metrics devised to assess which journal is better regarding this metric (“Journal Metrics - Impact, Speed and Reach,” n.d.). There are no publications related to the reproducibility topic which compare different journals and languages. Although there are some exploring reproducibility within one specific journal (Stodden, Seiler, and Ma 2018). What is more, journals notice the importance of this subject (McNutt 2014). Also according to scientists journals should take some responsibility for this subject (Eisner 2018). 1.8.3 Methodology We decided to focus on three journals: The Journal of Statistical Software The Journal of Machine Learning Research The Journal of Open Source Software The Journal of Statistical Software and The Journal of Machine Learning Research are well known among scientists in the field of data science. The Journal of Open Source Software is relatively new, was established in 2016. We choose articles randomly from the time frame 2015-present. From every journal we choose 10 articles, of which 5 are articles introducing an R package and 5 are introducing a Python library. We choose only articles having tests on their github repositories. For our metrics we test the following: Tests on github provided by authors - for R packages test_that tests, for Python libraries pytest and unittest tests. Examples from the article - we test whether chunks of code included in the article produce the same results. Number of examples in an article varies a lot, in particular all the articles from Journal of Open Source Software do not have any examples. Examples provided by the authors on github repository in the catalog examples. 1.8.3.1 How to compare articles? 1.8.3.1.1 Insight to the problem Before anything can be said about the differences in journals and languages, first there has to be a measure in which they can be compared. Journals in general prefer articles of the same structure. What it means is that articles from different journals can vary substantially. This includes not only topics but number of pages, style of writing and most importantly for the topic of this article the way they present code. Thus it comes as no surprise that there are many means how the code can be reproduced. Every so often when an article is presenting a package there can be no examples and only unit tests. Naturally, the opposite can occur. Obvious conclusion is that the proposed measure must not be in favor of any way of presenting code in the given article. The problem of defining the right measure of article reproducibility deserves a separate article itself and It should be stated that metrics used by us are for sure not without a flaw. We do not assume that they are unbiased but that they are true enough that we can draw conclusions from them. 1.8.3.1.2 Proposed metrics First of all, we did all tests provided by the author in the article or located on an online repository. But if there was an example but there was no direct connection to if from the article it was not included in our reproduction process, because in our opinion it’s not a part of the article and therefore journal. Because of what has been said in the previous paragraph we decided to look at articles from two perspectives. One is more bias, second is true to the sheer number of reproducible examples and positive tests. 1.8.3.1.3 Student’s Mark Analysis of the problem has led to the decision that numerical assessment of article reproducibility has too many flaws and does not represent well the problems that occur while recreating the results of an article. What we propose is a 4-degree mark ranging from 2 to 5. The highest mark, 5 is given if the author provided all results to the code shown in the article and repository and if the results can be fully reproduced. The article is scored 4 when there are some minor problems in the reproducibility of the code. For example, an article may lack the outputs to some part of the code shown or there are some errors in tests. The major rule is that code should still do what it was meant to do. If some errors happen, but they are not affecting the results, they are negligible. The article is scored 3 in a few cases. If an article lacks all or the vast majority of code outputs, but when reproduced it still produces reasonable results. When in some tests or examples we can observe non-negligible differences, but this cannot happen to a key element of the article. For example, the method proposed in the article describing training machine learning model works and the model is trained well, but there are errors in the part of the code where the model is used by some different library. If we would have to score the article based only on this example we would give it a 3. The article is scored 2 if there are visible differences in reproducing results of key elements of the article. Or If the code from the article didn’t work even though we had all dependencies. 1.8.3.1.4 Reproducibility value Second metric we used to analyse articles is simple and puts the same weight to the reproducibility problems of the tests and examples. \\[ R_{val} = 1 - \\frac{negative \\ tests + negative \\ examples}{all \\ tests + all \\ examples} \\] So a score of 0 represents an article that failed in all tests and had only not working examples. 1.8.4 Results Results of reproducing all chosen articles are presented in the following table: Title Journal Language StudentsMark ReproducibilityValue Autorank: A Python package for automated ranking of classifiers JOSS Python 4 0.95 Beyond Tandem Analysis: Joint Dimension Reduction and Clustering in R JSTAT R 5 1 CoClust: A Python Package for Co-Clustering JSTAT Python 2 0.3 corr2D: Implementation of Two-Dimensional Correlation Analysis in R JSTAT R 4 1 frailtyEM: An R Package for Estimating Semiparametric Shared Frailty Models JSTAT R 4 0.79 Graph Transliterator: A graph-based transliteration tool JOSS Python 4 0.93 iml: An R package for Interpretable Machine Learning JOSS R 5 1 learningCurve: An implementation of Crawford’s and Wright’s learning curve production functions JOSS R 5 1 mimosa: A Modern Graphical User Interface for 2-level Mixed Models JOSS R 3 0.67 mlr: Machine Learning in R JMLR R 4 0.98 OpenEnsembles: A Python Resource for Ensemble Clustering JMLR Python 2 0.78 origami: A Generalized Framework for Cross-Validation in R JOSS R 5 1 py-pde: A Python package for solving partial differential equations JOSS Python 2 0.86 PyEscape: A narrow escape problem simulator packagefor Python JOSS Python 2 0.8 Pyglmnet: Python implementation of elastic-net regularized generalized linear models JOSS Python 5 0.98 pyParticleEst: A Python Framework for Particle-Based Estimation Methods JSTAT Python 3 0.67 “PypeR, A Python Package for Using R in Python” JSTAT Python 3 1 “Rclean: A Tool for Writing Cleaner, More Transparent Code” JOSS R 5 1 RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research JMLR Python 2 0.59 rmcfs: An R Package for Monte Carlo Feature Selection and Interdependency Discovery JSTAT R 2 0.57 Seglearn: A Python Package for Learning Sequences and Time Series JMLR Python 4 0.88 tacmagic: Positron emission tomography analysis in R JOSS R 5 1 TensorLy: Tensor Learning in Python JMLR Python 3 1 The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R JMLR R 3 1 The huge Package for High-dimensional Undirected Graph Estimation in R JMLR R 3 1 To better present obtained results plots below show distribution of marks within each journal and language: FIGURE 1.5: Distribution of ‘Student’s Mark’ score of reproduced articles within each journal. FIGURE 1.6: Distribution of ‘Student’s Mark’ score of reproduced articles within each language. Following plots show means of ‘Student’s Mark’ scores of articles within each journal and each language: FIGURE 1.7: Comparison of mean ‘Student’s Mark’ score of reproduced articles between journals. FIGURE 1.8: Comparison of mean ‘Student’s Mark’ score of reproduced articles between languages. Based on the plots we can see that R articles had a better mean score and Journal of Open Source Software had also the best mean score among the journals. Similar plots below show means of ‘Reprodcibility Value’ scores: FIGURE 1.9: Comparison of mean ‘Reproducibility Value’ score of reproduced articles between journals. FIGURE 1.10: Comparison of mean ‘Reproducibility Value’ score of reproduced articles between languages. Same as with ‘Student’s Mark’ we can see that R articles had a better mean score and Journal of Open Source Software had the best mean score among the journals. 1.8.5 Summary and conclusions References "],
["imputation.html", "Chapter 2 Imputation", " Chapter 2 Imputation Imputation "],
["default-imputation-efficiency-comparision.html", "2.1 Default imputation efficiency comparision", " 2.1 Default imputation efficiency comparision Authors: Jakub Pingielski, Paulina Przybyłek, Renata Rólkiewicz, Jakub Wiśniewski (Warsaw University of Technology) 2.1.1 Abstract 2.1.2 Introduction and Motivation 2.1.3 Related Work 2.1.4 Methodology 2.1.5 Results 2.1.6 Summary and conclusions "],
["the-hajada-imputation-test.html", "2.2 The Hajada Imputation Test", " 2.2 The Hajada Imputation Test Authors: Jakub Kosterna, Dawid Przybyliński, Hanna Zdulska (Warsaw University of Technology) 2.2.1 Abstract There are many different ways of dealing with missing values. Depending on their quantity and properties, various methods turn out to be the best. The fact is, there is no one universal best method of imputation, and different problems require various solutions. Using one and the same solution for each dataset is certainly not the best option, but sometimes it is convenient or even necessary to focus on one particular method without much insight into its effect - for example, when time limits us or when, due to the complexity of the algorithm or class of problems, we are forced to choose only one method. In this chapter which we decided to name The Hajada Imputation Test (taken from the first two letters of creators’ names) we will take a look at several popular imputation methods and try to compare them with each other, including ranking of algorithms taken into account and finally choosing the best one. Obviously its result will not be the final verdict declaring which imputation algorithm is clearly better or worse, but considering the comparison of their quality on different datasets, we will be able to assess their collective performance and effectiveness against the backdrop of the whole. For this purpose, we will test the modified collections for getting rid of missing values on several machine learning classification algorithms, and then considering the selected quality measures after comparing the results, we will get the desired view. Due to the need to obtain a clear and interpretable result, we have limited ourselves to the problem of binary classification. 2.2.2 Introduction and Motivation In statistics, imputation is the process of replacing missing data with substituted values. Over the years, humanity has created many different methods accomplishing that, including the simple and instinctive ones but also more advanced and hard to be easily explained. Choosing the best-suited imputation method for the dataset with missing values is still the daily dilemma of every data scientist and there isn’t one universally recognized best technique. Some scientists believe the crux lies in the most advanced and sophisticated algorithms, others trust the simplest of all possible. In a way, both parts are right - the first naturally win in speed of execution, but the second ones do not have advanced operations implemented for nothing and this usually results in data that more accurately reflects reality or what might be expected from it. How do the individual popular ways fall on the famous collections from OpenML1001? Many wondered, we decided to find the answer to this question. Many were wondering, but few answered this question - some interesting conclusions have been developed here. 2.2.2.1 Imputation functions Six different methods have been taken into consideration. Their selection is well thought out - they are all widely known in the world of Data Science, and at the same time they differ significantly in approach, implementation, concept and results. Comparing them not only results in a fair view of the strictly indicated implementations, but also what result subsequent approaches result in. Three simple imputation methods and three more advanced algorithms offered by popular packages were taken to consideration and their comparison. The clash of implementations with such a degree of diversity of complexity gives a clear message whether it pays off to reach for an advanced algorithm from the package prepared by professionals for many hours, conducted on many tests, or rather stay with minimalism and write easy code solving the problem in a trivial way. Methods compared in subsequent stages are: mode / median replace - basic process which puts in a place of missing data median cells from their columns for numerical values and dominants for categorical ones. remove rows - trivial solution of removing rows containing any missing data. ‘bogo’ random replace - simple algorithm replacing NA values with random numbers or factors from their features. mice 2 - advanced method creating multiple imputations for multivariate missing data, based on Fully Conditional Specification 3, where each incomplete variable is imputed by a separate model. Standard imputation with maxit parameter of value 0 and default 5 number of imputed datasets. VIM 4 - standard k-nearest neighbors algorithm taken from the library. missForest 5 - imputation based on random forest offered by the package. 2.2.2.2 Datasets The fourteen data frames were taken from OpenML100 collection 1 and were corrected specifically for this research. Both small and simple frames were chosen as well as more complex and containing a lot of information, difficult to explain or present. They can be found under the following identifiers with the following names: 1590 - adult, 188 - eucalyptus, 23381 - dresses-sales, 29 - credit-approval, 38 - sick, 40536 - SpeedDating, 41278 - okcupid-stem, 56 - vote, 6332 - cylinder-bands, 1018 - ipums_la_99-small, 27 - colic, 4 - labor, 55 - hepatitis and 944 - echoMonths. These above have been placed in individual directories identified by id in the prepared directory. The six imputations mentioned earlier were conducted on all fourteen of them, but only on six imputations were successful in their entirety. 2.2.2.3 Binary classification algorithms In order to compare the quality of the imputed data and see how this supplementation of deficiencies works in practice, five binary classification algorithms have been selected. Their choice was made after careful analysis and extensive discussion, in order to find models that are both widely known and used, but also apply to different approaches and give reasonably distinguishable results using different techniques. The final choice fell on: classification tree from rpart6 - classic algorithm which uses a decision tree to go from observations about an item to conclusions about the item’s target value, k-Nearest Neighbors7 from class8 package - standard knn7 model attributing a given observation to a target corresponding to its closest observations in space, naive Bayes from e10719 - well-known classification computing the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule. random forest from ranger10 - algorithm consisting of many decisions trees uses bagging and feature randomness when building each individual tree trying to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. Support Vector Machine from e10719 - a discriminative classifier formally defined by a separating hyperplane. 2.2.3 Methology 2.2.3.1 Reading datasets In order to read all datasets another function was used, which returned a dataframe containing all the important informations about the test matrices. The generated frames of imputation results and times were saved using parallel processing11. 2.2.3.2 Metrics functions In order to test the same test-train splits we used random seed 1357 for all datasets. For each machine learning model after every imputation was created confusion matrix and the values of four basic metrics: accuracy - \\(\\frac{TP+TN}{TP+FP+FN+TN}\\) precision - \\(\\frac{TP}{TP+FP}\\) recall - \\(\\frac{TP}{TP+FN}\\) f1 - \\(2*\\frac{Recall * Precision}{Recall + Precision}\\) For the final conclusion, ranking concerned only the last one. Additionally, Matthews correlation coefficient12 measures were also computed. \\({\\displaystyle {\\text{MCC}}={\\frac {{\\mathit {TP}}\\times {\\mathit {TN}}-{\\mathit {FP}}\\times {\\mathit {FN}}}{\\sqrt {({\\mathit {TP}}+{\\mathit {FP}})({\\mathit {TP}}+{\\mathit {FN}})({\\mathit {TN}}+{\\mathit {FP}})({\\mathit {TN}}+{\\mathit {FN}})}}}}\\) These two give great information about the quality of imputation and a good comparison. It resulted in simple and clear information about which observations are well classified, which ones are wrong and how they should be. The main advantage of them is accounting for target variable’s inbalance. Considering sets’ different properties, even when it comes to balancing, comparing those measures, the fact that they show the pros and cons of given result will additionally be worthwhile to compare them. 2.2.4 Results 2.2.4.1 Comparing imputation times A dataframe containing information about imputation methods was built. Judging by the logarithmic scale, there is no surprise that removing rows and mode/median replenishment are definitely the fastest methods, with removing rows being several times faster. What would you expect, also “random values” imputation was quite fast, but slower than the two mentioned. Looking at more advanced ones, definitely VIM turned up to be the fastest - times usually being approximately 5-10 times shorter than missForest and mice. These last two are quite slow, with the former appearing to be slightly faster. For a better view a boxplot was also created: Taking into account all the imputations which have been implemented, it is clearly seen that removing rows as well as replacing with mode or median really stand out. However, we cannot fully compare the other four - due to the fact that missForest failed on three datasets, and mice - up to seven, mostly for datasets greater in size. Considering such a large spread of data size, there is a very interesting difference between median and mean for VIM’s time - the first is almost seven minutes, the second - barely a second and a half. In general, however, it is certainly much longer than methods for removing incomplete rows and filling with mode/median. Also time was compared for those datasets for which all imputations were successful - these are ids 27, 38, 55, 56, 944, 188 (dataset with id 4 softened on removing rows containing any missing items, because each of its poems had some missing items). Taking into account the mean time, definitely missForest was the slowest, but also its standard deviation seemed to be incomparably large - this is probably due to the fact that for smaller sets it is doing well, but due to its complexity, its slowdown can be seen for very sizeable datasets. VIM turned out to be better for quick calculations than missForest and mice, and considering the median, mice is comparable to missForest - so for small datasets there is not much difference between them, and a lot of time was definitely needed to devote to these larger data frames. 2.2.4.2 Best measures In order to compare F1 and MCC measures, mean results on all sets for all imputations were calculated. Then, for each machine learning algorithm a ranking was created - so that the imputation with best results obtained the first rank, second best - 2nd, etc. The rank’s distributions for every imputation came out as shown by the following boxplot: It might seem that the results of the methods are very close to each other, but the above visualization takes into account rankings only for successful imputations - to get the correct comparison there is a need to make a ranking with failing methods receiving the worst ranks instead of being omitted. The outcome was presented below: Considering the imputation times and results, it can be said that the knn7 algorithm from the VIM package proved to be “universally” the best. Out of the advanced methods it falls out the fastest, and its results are also the best. However, it is more difficult to choose second and subsequent places - looking at the results, the simple methods are very quick, but they differ from the more advanced ones. Looking at the advanced ones, with the exception of VIM, MissForest stands out. However, his problem is time. So after VIM, it seems second best to use mean / median replenishment for a small amount of time and missForest for a large one. However, although mode / medain usually obtained worse ranks, there was no extensive difference in results between the methods - which is not visible in the charts. “removeRows” is characterized by large fluctuations, and therefore we advise against using it. 2.2.4.3 Summary and conclusions The experiment can be considered successful when it comes to the datasets and tools made available to us - calculated measures, visualizations made, as well as conclusions led us, among others, to the intuition that probably VIM is really a resplendent package, supplementing the missing values with medians from columns is not is such an unwise idea as it may seem and missForest is also noteworthy - if one has time for running it and the dataset isn’t too sizeable. Unfortunately, it is impossible to draw an objective conclusion based on such a small number of tested data - with as few as 14 sets, problems with imputations on some of them or too small dataframes not giving a satisfactory answer. Nevertheless, the code is certainly very valuable and for more datasets it could confirm one or another belief. 2.2.4.4 References 1OpenML100: a predecessor of the OpenML-CC18, a suite of 100 classification datasets from OpenML which were carefully selected to be usable by many algorithms and also represent datasets commonly used in machine learning research 2mice: Multivariate Imputation by Chained Equations, package created by Stef van Buuren, Karin Groothuis-Oudshoorn, Gerko Vink, Rianne Schouten, Alexander Robitzsch, Lisa Doove, Shahab Jolani, Margarita Moreno-Betancur, Ian White, Philipp Gaffert, Florian Meinfelder, Bernie Gray 3Fully conditional specification: Imputes multivariate missing data on a variable-by-variable basis (Van Buuren et al. 2006; Van Buuren 2007a). 4VIM: Visualization and Imputation of Missing Values, package created by Matthias Templ, Alexander Kowarik, Andreas Alfons, Bernd Prantner 5missForest: Nonparametric Missing Value Imputation using Random Forest Daniel J. Stekhoven 6rpart: Recursive Partitioning and Regression Trees, package by Terry Therneau, Beth Atkinson, Brian Ripley 7k-nearest neighbors algorithm: a non-parametric method with the input consisting of the k closest training examples in the feature space. 8class: Various functions for classification, including k-nearest neighbour, Learning Vector Quantization and Self-Organizing Maps, package created by Brian Ripley, William Venables 9e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien, package created by David Meyer 10ranger: A Fast Implementation of Random Forests, package created by Marvin N. Wright, Stefan Wager, Philipp Probst 11parallel: Support for parallel computation, including by forking (taken from package multicore), by sockets (taken from package snow) and random-number generation 12Matthews correlation coefficient: a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975 11Matthews correlation coefficient: a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975 "],
["comparison-of-performance-of-data-imputation-methods-in-the-context-of-their-impact-on-the-prediction-efficiency-of-classification-algorithms-.html", "2.3 Comparison of performance of data imputation methods in the context of their impact on the prediction efficiency of classification algorithms.", " 2.3 Comparison of performance of data imputation methods in the context of their impact on the prediction efficiency of classification algorithms. Authors: Ada Gąssowska, Mateusz Grzyb, Elżbieta Jowik (Warsaw University of Technology) 2.3.1 Abstract It is very common for datasets to contain incomplete observations. The analysis conducted by ignoring cases with missing data and considering only complete instances is perceived as naive, inefficient and exposed to bias, as incomplete ones may also convey valuable information. To incorporate these sketchy observations, various methods of handling missing data may be considered, including both less and more sophisticated approaches. It turns out, that the performance of machine learning classification models moderately depends on the type of approach that is applied to the imputation problem, while the time of the imputation, when using different techniques, is highly diverse. During this study, we considered ten data sets, six imputation methods and four classifiers. Mentioned imputation methods contain one basic technique (median/mode imputation) and five more sophisticated ones, which origin respectively from mice, VIM, missRanger and softImpute R packages. For testing purposes, as the classification algorithms, we used Ranger Random Forests, XGBoost, K Nearest Neighbors and Naive Bayes classifiers. To evaluate performance of each imputation method/classification algorithm combination on a given dataset, we used raw Area Under the Curve (AUC), Balanced Accuracy Score (BACC) and Matthew’s Correlation Coefficient (MCC) measures, as well, as rankings based on them. Times of each imputation were also verified and compared. The actual differences in prediction correctness are slight in general. In respect to the rankings, for the Ranger Random Forests, XGBoost and Naive Bayes classification algorithms the missRanger imputation method turned out to be the best, while for the K Nearest Neighbors - it was the basic one. As for the time of imputation, it highly depends on the technique complexity, so the basic method turns out to be the fastest one. However, the hotdeck method from the VIM package is also noteworthy, as it performs only a little worse. The analysis shows, that selecting the best imputation method/classification algorithm combination, that will work best globally, is extremely difficult, because a lot depends on the structure of the dataset, the prediction correctness assessment technique and the amount of computational power available. 2.3.2 Introduction and motivation Dealing with missing data is a substantial part of feature engineering, as most of the machine learning algorithms do not accept incomplete datasets. Data imputation, especially replacing missing values with a value based on other available cases, is the solution for that problem. However, despite the acknowledged importance of it, in many practical cases it is not handled with proper caution. Basic median/mode imputation is often used, as well as deleting rows with missing data (if the number of missing values is low). As there are many different imputation techniques, choosing which one to use is complicated, but it may be beneficial. There are no objective rules to follow, the only way to choose the best one is to perform numerous comparisions. The purpose of our experiment is to find best ways to impute data while using specific classification algorithms. 2.3.3 Methodology Most of the algorithms that we used for tests do not accept missing values. To find, which one from all tested imputation methods is best for each tested classification algorithm, we decided to perform the following experiment. Firstly, we created six different imputation functions: basic method using impute() function from imputeMissings package. It imputes missing numeric values with median and categorical variables with mode of other records. mice() function from mice package (Buuren and Groothuis-Oudshoorn 2011). The function uses Predictive Mean Matching to impute missing values. missRanger() function from missRanger package (Mayer 2019). The technique uses the ranger package to do fast missing value imputation by chained random forests. hotdeck() function from VIM package (Kowarik and Templ 2016a). Each missing value is replaced with an observed response from a “similar” unit. kNN() function from VIM package (Kowarik and Templ 2016a). It finds the k closest neighbors to the observation with missing data and then imputes them based on the the non-missing values from its neighbors. softImpute() function combined with mode imputation (Hastie and Mazumder 2015a). The first method origins from softImpute package and is applied to impute numeric variables. Mode imputation is used for categorical features imputation. To automatize our work, we also created two specialized functions - split_and_impute() and train_and_test(). The first function, divides given dataset into train and test sets (with configurable proportions and with stratification on the target variable) and imputes it with specified imputation function (one of the above). It returns imputed train and test sets and the time of the imputation (all of which are can be saved to files autoamtically). The second function, performs crossvalidation on train set (the default number of folds is 5) and makes the predictions for the test set (that is, after training the model on the whole train set), with a specified classification algorithm. Target variable and label of the positive class must also be passed before function call. Based on mentioned predictions, it calculates AUC (Flach Peter 2011), BACC (Velez and Moore 2007) and MCC (Boughorbel S. 2017) measures for both crossvalidation and test set testing stages. It also returns plots of ROC curve, AUC, BACC, and MCC measures achieved during crossvalidation stage. All ten benchmarking datasets were obtained from https://www.openml.org/ and can be found there with detailed descriptions. Then, we proceeded in the following way: Firstly, all benchmarking datasets where split and imputed with all imputation methods using split_and_impute() function. All the resulting subsets were then saved to corresponding files. This way, all results differences observed later did not depend on any splitting or imputation randomness, as these steps were taken only once for each benchmarking dataset. Time of each imputation was also measured and taken into consideration. Secondly, for all imputation method/dataset combination train_and_test() function was called four times - once for each classification algorithm. All returned values were then used for two purposes: to create a specialized report (available here - https://github.com/PlentyCoups/WB-article-extras) used for checking process correctness (mostly by looking at crossvalidation scores variances) and, more importantly, to create aggregated test set scores plots for further inference. Following diagram may also help to visualize our methodology: Figure 1: Methodology diagram 2.3.4 Results Raw results were proccesed and assessed in the following way. Imputation times were compared in correspondence to each imputation method and dataset. Times logarithms had to be used instead of raw data, due to high differences. (Figure 2) Raw measures were compared in correspondence to each imputation method, classification algorithm and dataset. (Figure 3) Rankings of measures were compared in correspondence to each imputation method, classification algorithm and dataset. Each imputation method/classification algorithm combination was ranked on every dataset in based on one of three measures. Given combination gets 1 point if it gives the best result on a dataset, and 6 points if it gives the worst one. Then, such rankings were visualized through multiple boxplots with mean and median ranking marked on them. (Figure 4) Moreover, as it was already mentioned before, crossvalidation scores were merged in a specialized report used for checking process correctness, that is available here - https://github.com/PlentyCoups/WB-article-extras. It should be mentioned once again, that any differences in results are only due to a change of either the imputation method, the classification algorithm or the dataset. Divisions into train and test sets for a given dataset were always identical. Because of that, even small scores differences should be taken into consideration. Another important fact is that we took into consideration only datasets on which all of the imputation methods worked, so that all of them may be compared fairly. Figure 2: Imputation times Figure 3: Raw scores Figure 4: Scores rankings 2.3.5 Summary and conclusions After analysing the results, especially the plots, it became clear, that it is not possible to pick the best imputation method for all classification alghoritms. For K nearest neighbours classifier the best AUC, BACC and MCC scores were gained for basic median/mode imputation methid. For three other alghoritms (Naive Bayes, XGBoost and Ranger Random Forest) the missRanger imputation turned out to be the best one, but the basic median/mode imputer was second in most of the rankings. It was also the quickest one as it can be seen on Figure 2. The imputer that turned out to be globally the worst, and also the slowest, was the one from mice package. It is very important to note, that even though we we can pick globally best and worst techniques, most imputation method/classification algorithm combinations ended first on at least one dataset. Because of that, if best possible prediction corectness is an absolute priority, it is always the best to consider all available options. In conclusion, it is a good idea to use basic median/mode imputation as a solid and fast to obtain starting point, and then, when possible and desirable, to evaluate more sophisticated methods, in hope of better results. References "],
["various-data-imputation-techniques-in-r.html", "2.4 Various data imputation techniques in R", " 2.4 Various data imputation techniques in R Authors: Jan Borowski, Filip Chrzuszcz, Piotr Fic (Warsaw University of Technology) 2.4.1 Abstract There are many suggestions on how to deal with missing values in data sets problem. Some solutions are offered in publicly available packages for the R language. In our study, we tried to compare the quality of different methods of data imputation and their impact on the performance of machine learning models. We scored different algorithms on various data sets imputed by chosen packages. Results summary presents packages which enabled to achieve the best models predictions metrics. Moreover, the duration of imputation was measured. 2.4.2 Introduction and Motivation 2.4.2.1 Background and related work Missing observations in data sets is a common and difficult problem. In the field of machine learning, one of the key objects is the data set. Real-world data are often incomplete, which prevents the usage of many algorithms. Most implementations of machine learning models, available in popular packages, are not prepared to deal with missing values. Before creating a machine learning model, it is essential to solving the problem of missing observations. This requires user pre-processing of data. Some researches examined the similarity between original and imputed data, in terms of descriptive statistics (Musil et al. 2002). Missing data are common in medical sciences and the impact of different imputation methods on analysis was measured (Bono et al. 2007). Some studies show that imputation can improve the results of machine learning models and that more advanced techniques of imputation outperform basic solutions (Batista and Monard 2003) (Su, Khoshgoftaar, and Greiner 2008). 2.4.2.2 Motivation Various imputation techniques are implemented in different packages for the R language. Their performance is often analyzed independently and only in terms of imputation alone. Because of a variety of available tools, it becomes uncertain which one package and method to use, when a complete data set is needed for a machine learning model. In our study, we would like to examine, how methods offered by some popular packages perform on various data sets. We want to consider the flexibility of these packages to deal with different data sets. The most important issue for us is the impact of performed imputation on later machine learning model performance. We are going to consider one specific type of machine learning tasks: supervised binary classification. Our aim is a comparison of metrics scores achieved by various models depending on the chosen imputation method. 2.4.2.3 Definition of missing data In the beginning, clarifying the definition of missing data is necessary. Missing data means, that one or more variables have no data values in observations. This can be caused by various reasons, which we can formally define as follows, referring to Rubin (1976): MCAR (Missing completely at random) Values are missing completely at random if the events that lead to lack of value are independent both of observable variable and of unobservable parameters. The missing data are simply a random subset of the data. Analysis performed on MCAR data is unbiased. However, data are rarely MCAR. MAR (Missing at random) Missingness of the values can be fully explained by complete variables. In other words, missing data are not affected by their characteristic, but are related to some or all of the observed data. This is the most common assumption about missing data. MNAR (Missing not at random) When data are missing not at random, the missingness is related to the characteristic of the variable itself. 2.4.2.4 Techniques of dealing with missing data In case of preparing a data set for machine learning models, we can generally distinguish two approaches. The first method is omission. From the data set, we can remove observations with at least one missing value or we can remove whole variables where missing values are present. This strategy is appropriate if the features are MCAR. However, it is frequently used also when this assumption is not met. It is also useless when the percentage of missing values is high. The second approach is imputation, where values are filled in the place of missing data. There are many methods of imputation, which we can divide into two groups. Single imputation techniques use the information of one variable with missing values. A popular method is filling missings with mean, median or mode of no missing values. More advanced are predictions from regression models which are applied on the mean and covariance matrix estimated by analysis of complete cases. The main disadvantage of single imputation is treating the imputed value as true value. This method does not take into account the uncertainty of the missing value prediction. For this reason multiple imputation was proposed. This method imputes k values, which leads to creating k complete data sets. The analysis or model is applied on each complete data set and finally, results are consolidated. This approach keeps the uncertainty about the range of values which the true value could have taken. Additionally, multiple imputation can be used in both cases of MCAR and MAR data. 2.4.3 Methodology Experiment like this one can be performed involving many techniques we decide to divide our tests into 4 steps: Data Preparation, Data Imputation, Model Training, Model Evaluation. Below we will explain every step in detail. 2.4.3.1 Data Preperation For test purposes we used 14 data sets form OpenML library (Casalicchio et al. 2019). Every data set is designed for binary classification and most of them contain numerical and categorical features. Most of the sets have a similar number of observations in both classes but some of them were very unbalanced. Before data imputation data set was prepared specific preparation are different for each data set but we commonly do: Removing features which didn’t contain useful information (for example all observation have the same value) Correcting typos and converting all string to lower case to reduce the number of categories Converting date to more than one column (for example “2018-03-31” can be converted to three columns: year, month and day) Removing or converting columns with too many categories After cleaning data sets were transferred to the next step. 2.4.3.2 Data Imputation Clean data sets were split into two data sets, training and testing, in proportion \\(1/4\\) respectively. This split was performed randomly and only once for every data set that’s mean every imputation technique used the same split. Imputation was performed separately for train and test sets. Before split, we also remove the target column to avoid using it in imputation. For our study, we decided to choose five packages designed for missing data imputation in the R language and one self-implemented basic technique: Mode and median: Simple technique of filling missing values with mode (for categorical variables) and median (for continuous variables) of complete values in a variable. Implemented with basic R language functions. mice(van Buuren and Groothuis-Oudshoorn 2011): Package allows to perform multivariate imputation by chained equations (MICE), which is a type of multiple imputation. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. missMDA(Josse and Husson 2016): Package for multiple missing values imputation. Data sets are imputed with the principal component method, regularized iterative FAMD algorithm (factorial analysis for mixed data). Firstly estimation of the number of dimensions for factorial analysis is essential. missFOREST(Stekhoven and Buehlmann 2012): Package can be used for imputation with predictions of the random forest model, trained on complete observations. The package works on data with complex interactions and non-linear relations. Enables parallel calculations. softImpute(Hastie and Mazumder 2015b): Package for matrix imputation with nuclear-norm regularization. The algorithm works like EM, solving an optimization problem using a soft-thresholded SVD algorithm. Works only with continuous variables. VIM(Kowarik and Templ 2016b): Package for visualization and imputation of missing values. It offers iterative robust model-based imputation (IRMI). In each iteration, one variable is used as a response variable and the remaining variables as the regressors. First, we use median/mode imputation, which is a very simple method and it is used as a base result for more complex algorithms to compare. Imputation method form mice package don’t require any form of help because can impute numeric and categorical features. SoftImpute package works only with numeric features. To compare it with other algorithms on the same data we use softImpute for numeric variables and mode for categorical variables. Alternatively, it is possible to use SoftImpute for numeric features and different algorithms for categorical variables, but we decided that this approach may lead to unreliable results. MissForest algorithm can be used on both numeric and categorical features and is capable of performing imputation without any help of other methods. Imputation method from Mice package also can be run on all types of data. Iterative Robust Model-Based Imputation method from VIM package can impute all types of data. This method additionally creates new columns with information whether the observation was imputed or not. We decided to do not use these columns, because other methods do not create them. The last method which we covered is missMDA which also can be used to input numeric and categorical features. After imputation, we add back target variable to both sets. All methods work on the same parameters for all data sets. In case when for some reason method can’t input some data set it was treated like “worst result” more information about it can be found in section 4. Model evaluation. 2.4.3.3 Model traing For classification task we use four classification algorithms: Extreme Gradient Boosting, Random Forest, Support Vector Machines, Linear Regression All methods were implemented in mlr package (Bischl et al. 2016a) for hyperparameters tuning, we also used methods from the same package. For all data sets, four classifiers were trained and tuned on the same train sets. To select parameters we used Grid Search. We will not focus on this part of the experiment. The most important part of this step is that every model training was carried out the same way. This means that differences in results can be caused only by the influence of previously used imputation technique. 2.4.3.4 Model evaluation After previous steps, we have got trained models and test sets. In the final, step we evaluate model and imputation. For every imputation and algorithm we calculate F1 score expressed by formula \\(2\\frac{(precision)\\cdot(recall)}{precision + recall}\\) and accuracy. In case when imputation algorithm fails to impute some data set, results for this set are thread as “the worst result”. It means if you try to create a ranking it is always last (if more then one imputation fail, all of them is placed last). A detailed discussion about results in the next section. 2.4.4 Results After the long and tedious process of data imputation, it is finally time to evaluate our methods of imputation. Methods were trained and tested on 14 data sets and evaluated strictly using 2 methods mentioned earlier. Before score analysis, it is worth mentioning that all algorithms were tested with optimal parameters, so the score should be quite meaningful. The table below presents average metrics achieved by model, depending on the imputation method. As described earlier, we decided to use two measures of algorithm effectiveness: F1 score Accuracy These two measures complement each other well because they allow us to measure well the effectiveness of our imputations and algorithms on both balanced and unbalanced sets. Method Accuracy F1 median 0.83 0.75 mice 0.85 0.75 missForest 0.85 0.74 missmda 0.82 0.80 softimpute 0.86 0.75 VIM 0.85 0.76 Our experiment did not find out the best imputation algorithm, but we can derive some interesting conclusions from it. First of all, we can think about the median/mode imputation as kind of our baseline because it will be useful later. Surprisingly it seems to perform quite well, among the others, often quite sophisticated methods. It achieved over 80% of accuracy on average. However, it is hard to decide whether it is a high score or not, because we are only able to compare algorithms between the others. To say anything more meaningful about our methods we shall look at the distribution of the scores to make a better analysis of performance. It is also worth noting that not all algorithms managed to perfomrm imputations on all datasets, but differences in that issue were so little, that tey will not affect our scores. 2.4.4.0.1 Distributions of scores Taking a brief look at the distributions of accuracy score it seems quite unclear which algorithm performs the best. All medians seem to be approximately on the same level and also the first and third quartile of almost all of the plots have the same value. Only missMDA is a bit lower than the others, but this is too early to derive any conclusions. F1 scores give us much more information. The first thing that becomes apparent after looking at that plot is the fact that we do have some very low values for every type of imputation. However, this is simply because some of our data sets were very small, saw neither of our algorithms was able to achieve recall above 0. Besides from that once again scores of all algorithms were quite close to each other. This is why we decide to use two different methods of comparing and classifying these algorithms so that we will be able to choose our winner. 2.4.4.1 Median/mode baseline score So to begin with we decided to treat the median/mode as the basic form of imputation and we will compare it to all other methods. We want to check how much the average prediction measured made by other algorithms differed from the median/mode. As one prediction, we understand the average F1 score made by of all imputation methods on one data set. Method score softimpute 0.02 missForest 0.01 mice -0.02 missmda -0.11 VIM -0.11 Results achieved here are quite surprising. First of all the maximum gain in F1 score is only 0.02 and it is achieved by softImpute. But what can bewilder the most is the fact that only 2 out of 5 algorithms managed to perform better than the median/mode. Score achieved by SoftImpute may be in some way caused by the fact, that it is using median imputation in some situations. These scores can raise questions about the whole purpose of using sophisticated algorithms, but of course, we are unable to say anything harsh about them yet. We can definitely note that we need to be we careful while using these algorithms, because they may lead to huge loss in scores, even up to -0.11, as here it is shown with missMDA. It is worth noting the order of the algorithms here, as we will use another method to compare the scores. 2.4.4.2 Second approach As a second approach, we decide to simply rank each algorithm score on each data set, and then award points for each place. As a result, we sum up all of the points and the final score is the sum of the points across all data sets. Of course, the algorithm with the lowest amount of points wins. Method Score softimpute 38 median 39 missForest 48 mice 49 VIM 58 missmda 69 Scores achieved here resemble these achieved here. The order is not the same, but the winner remains the same. Once again median/mode imputation managed to perform very well, outperforming many complicated algorithms. We can say that our scores are stable, so conclusions taken from these scores can be taken seriously. 2.4.4.3 Times As a final tool of comparing algorithms, we decided to compare times of imputation. Obviously, even the algorithm with the best score, but with awful imputation time is considered useless, so this is important to take that factor to final evaluation. As we can see times of imputations made by each algorithm can vary heavily. There is no point in creating rankings like we did to analyze scores achieved by algorithms, but it is simply worth noting that our previous winner - softImpute is very quick, its plot looks the same as the median/mode plot, which is quite an achievement. On the other side, there is Mice, which times can reach very high values. 2.4.5 Summary and conclusions Summing up all of our work we can say, that we managed to find plenty of interesting things about all of these imputation algorithms and the way they deal with different data. However, it is hard to issue the final judgments, because we only had 14 data sets available. Despite that, something definitely can be said. First of all, it is worth noting how well simple methods of imputation have managed to perform. Median/mode imputation was a tough competitor and have been outperformed rarely in our tests. The second issue is the matter of choosing the optimal metric for checking the performance of imputation algorithms. We have shown 2 different approaches and scores they achieved differed slightly. Taking that into consideration we cannot say that we have a clear winner in our competition, but we can give some sort of advice for all interesting in imputing their data sets. The advice is quite simple, but what we have managed to show quite powerful. The case is to start imputing your data set with the most basic method, which is median/mode imputing, and then trying to beat its score with a different algorithm, for example softImpute, which in our tests managed to perform quite well. References "],
["comparison-of-efficiency-of-various-data-imputation-techniques.html", "2.5 Comparison of efficiency of various data imputation techniques", " 2.5 Comparison of efficiency of various data imputation techniques Authors: Martyna Majchrzak, Agata Makarewicz, Jacek Wiśniewski (Warsaw University of Technology) 2.5.1 Abstract 2.5.2 Introduction and Motivation 2.5.3 Related Work 2.5.4 Methodology 2.5.5 Results 2.5.6 Summary and conclusions "],
["interpretability.html", "Chapter 3 Interpretability", " Chapter 3 Interpretability Interpretability "],
["building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html", "3.1 Building an explainable model for ordinal classification on Eucalyptus dataset. Meeting black box model performance levels.", " 3.1 Building an explainable model for ordinal classification on Eucalyptus dataset. Meeting black box model performance levels. Authors: Karol Saputa, Małgorzata Wachulec, Aleksandra Wichrowska (Warsaw University of Technology) 3.1.1 Abstract Achieving higher and higher metrics results by using bigger and more complex models is becoming popular. However, it is still possible to build accurate models that are less complex. Hence, possible to explain and interpret easily. For a specific task of ordinal classification and exemplary dataset of Eucalypti, we provide a model (with AUC metric better than considered black box model) with an explanation of techniques applied. 3.1.2 1. Introduction and Motivation In the classification problems, the main goal is to map inputs to a categorical target variable. Most machine learning algorithms assume that the class attribute is unordered. However, there are many problems where target variable is ranked, for example while predicting movie ratings. When applying standard methods to such problems, we lose some informaton, which could improve our model performance. This paper presents various methods to make an explainable machine learning model for ordinal classification problem. The aim is to achieve better results than ‘black box’ model does. We will test some existing approaches to ordinal classification and take advantage of analysis and imputation of missing data, feature transformation and selection, as well as knowledge from exploratory data analysis. Our experiments are based on ‘eucalyptus’ dataset from OpenML. The dataset’s objective is to find the best seedlot for soil conservation in seasonally dry hill country. Predictions are made depending on features such as height, diameter and survival of the plants. Target variable is ordered - it is represented by values ‘low’, ‘average’, ‘good’ and ‘best’. This article is structured as follows: Section 2 provides a brief description of avaliable ordinal classification methods and existing black box to explainable model comparisons. Section 3 explaines the methodology used throughout our experiment. Section 4 shows the results obtained for various models and section 5 containes an explanation of the best achieved explaianble model. Section 6 sums up our findings. At the end, in section 7 there is a list of references we used in our research. 3.1.3 2. Related Work 3.1.3.1 2.1. Ordinal classification as a regression task As described in [2], one of the most fundamental techniques is to cast target labels to a sequence of numbers. For example \\(\\{low, good, best\\}\\) changes to \\(\\{0,1,2\\}\\). Then, standard regression can be applied. There is additional information about the classes in comparison to the usual nominal classification. Also, a metric used is different than in a classification task - mean square error used in a regression task takes into account similarities between two labels when a conversion is applied. 3.1.3.2 2.2. Transformation to multiple binary classification problems Ordinal classification taxonomy given in [2] specifies decompositions of classification task into binary problems as a separate technique to cope with ordinal classification. A more detailed algorithm and performance test are presented in [1] which suggests better scores obtained using binary decomposition than using a single model during a test on multiple datasets. Based on [1], as a binary decomposition we define a conversion of a target classes \\(\\{low, good, best\\}\\) into pairs of binary classes (True/False) \\(\\{\\{False, False\\}, \\{True, False\\}, \\{True, True\\}\\}\\) determining whether outcome is better than \\(low\\) and better than \\(good\\). After classification is made, probabilites for original classes are calculated as explained in [1], [2]. 3.1.3.3 2.3. Comparing black boxes and white boxes As presented in the Introduction and Motivation section, we aim to achieve good results in a machine learning task using an explainable and interpretable model (white box). A black box model is used as a baseline model. Detailed justification of that approach can be found in [4]. Herein our work further develops this issue by presenting a model with the possibility to extract, understand and reuse of its decision logic for a specific task of ordinal classification. 3.1.4 3. Methodology The aim of this article is to build the best interpretable model for an ordinal classification problem and comparing it to a black box model. First undertaken step was dividing the data into training and test sets, consisting of 70% and 30% of all data, respectively. Since the considered dataset has a categorical target variable, the division was done using random sampling within each class of the target variable in an attempt to balance the class distributions within the splits. Since results obtained by the models were dependent on the random seed that was used when dividing the sets, the results compared thoughout this article are averaged out over 100 different random seeds, assuring a valid comparison could be done. 3.1.4.1 3.1. Initial preprocessing In order to get a legitimate comparison, the data was initially preprocessed in such a way as to assure that both models’ performances are compared on the same test set. This initial preprocessing included: deleting the observations with ‘none’ value in the target variable from both the training set and the test set; deleting observations with missing values from test set, resulting in a 6% decease of the test set observations. It is important to note that missing values are still present in the training set. The reason why the missing data is deleted from the test set is that many of the explainable models cannot be run with missing data present. This means that the missing values will have to either be deleted or imputed later on. This leads to a possibility that the explainable model will impute missing data differently than the black box model, resulting in two different tests sets. And, if - instead of imputing missing data - we decide to delete it in order to make running explainable model possible, then the obtained test sets will differ in number of rows, making it impossible to draw any meaningful conclusions. Hence the missing data were deleted from the test set. 3.1.4.2 3.2. Running the black box model The black box model chosen for comparison is an extreme gradient boosting model. After the initial preprocessing the xgboost model was trained on the training set and used to predict results on the test set. As this model can only deal with numerical data, categorical (factor) variables were transformed using one hot encoding. The training proces and prediction were done using the mlr package in R, and the exact model specifications were the following: Category Specification Learner classif.xgboost from package xgboost Type classif Name eXtreme Gradient Boosting; Short name: xgboost Class classif.xgboost Properties twoclass,multiclass,numerics,prob,weights,missings,featimp Predict-Type response Hyperparameters nrounds=200,verbose=0,objective=multi:softmax The quality of prediction was measured using the AUC (area under ROC curve) measure. We have also used other metrics suitable for ordinal classification task [3] and the results of all of the models are presented in table in the Results section of this article. However, AUC is the measure that we used to assess and compare each considered model. The AUC measure achieved by the black box is the one we tried to top using a simpler explainable model. 3.1.4.3 3.3. Running the basic version the explainable model We have chosen a tree model to be the considered explainable model, its exact specifications were the following: Category Specification Learner classif.rpart from package rpart Type classif Name Decision Tree; Short name: rpart Class classif.rpart Properties twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp Predict-Type response Hyperparameters xval=0 As this model cannot be run with missing data, they were deleted from the training set before training the model. Another step was deleting one from each of the one-hot-encoded variables (the default function transforms variable with n factor levels into n columns, but n-1 columns are sufficient as the n-th column is a linear combination of the remaining n-1 columns). This model performed worse than the black box model - the outcomes are presented in the Results section of this article. The AUC measure achieved by this model provides the base for this research, to which other models’ results will be compared to. 3.1.4.4 3.4. Improving the explainable model As mentioned before, the explainable model was enhanced by applying existing approaches to ordinal classification, feature transformation and selection and missing data imputation. The refinement process consisted of, but was not limited to, the following: Splitting a multiclass classification problem into 3 binary classification problems - binary decomposition with 3 rpart models (as described in section 2.2) Changing the levels of the target variable:“low”, “average”, “good”, “best” into numeric values: 1, 2, 3, 4, respectively and running a regression rpart model. Imputing missing data in the training set. Selecting variables: deleting the site names and specific location tags. Transforming Latitude variable from factor to numeric. The fourth step has a scientific justification. The experiment for which the data was collected was focused on finding the best seedlot for soil conservation in seasonally dry hill country. All the data in this dataset comes from New Zealand, but there is a chance that the results of such experiment would be used for other geographical regions. So far our model was making the prediction based also on specific flat map coordinates and site names, that are present both in the training and the test set. This means it would be impossible to use this model for judging seedlots of eucalypti planted outside of New Zealand. To make this possible, we have decided to take away all the variables that give away the exact position of the seedlots, leaving features such as latitude and the characteristics of plants and their habitat. After each improvement the model was retrained and the results obtained on the test set were saved and compared with the previous version of the model. If the new change has improved the model’s performance on the test set then it became the base for further development. Instead, if it has not improved the model’s performance, the previous version of the model was being further developed. 3.1.5 4. Results Results obtained for each model are shown in the tables below: Explainable models: Model AUC MSE ACC ACC1 Percent Basic Explainable Model’s AUC Basic rpart 0.8259 0.5284 0.5835 0.9797 100.00% Three binary rparts 0.8430 0.5393 0.5816 0.9827 102.07% Regression rpart 0.8611 0.4995 0.5815 0.9321 104.27% Regression rpart with imputation 0.8598 0.5038 0.5798 0.9307 104.10% Regression rpart with no location 0.8613 0.4996 0.5815 0.9323 104.28% Regression rpart with no location and numeric lattitide 0.8612 0.4993 0.5816 0.9323 104.28% Black box models: Model AUC MSE ACC ACC1 Percent Basic Explainable Model’s AUC Xgboost with nrounds=5 0.8405 0.4998 0.6044 0.9830 101.77% Xgboost 0.8590 0.4467 0.6248 0.9873 104.00% We have visualised the AUC measure improvements, in comparison to the basic rpart (decision tree) model, on the graph below. As shown on the graph, after adding the improvements, the explainable model was able to outperform the black box model, which was the point of this study. The best model is a decision tree, which is defined for a regression task, and which includes variable selection (taking away specific location tags as explained in the methodology section). Results of different techniques in improving model performance in comparison to black box. The basic classification decision tree was used as a baseline. 3.1.6 5. Model explanantion Based on our final model a graph of decisions was created which is presented below. The most important, first divisions are made based on Vigour and Crown variables. Decisions can be reused to support other models or as a support for decision making e.g. in building environment for Eucalypti. Graph of a decision tree structure of our model 3.1.7 6. Summary and conclusions As mentioned before, three approaches to ordered classification were considered. On the studied Eucalyptus dataset, the ordinal classification as a regression task approach has given the highest AUC value. This could be due to the fact that the classes in the target variable were not equally distributed, and so the regression has performed better than multiclass classification or a set of binary classification problems. The final explainable model is a fairly simple decision tree, defined for a regression task, which includes variable selection. The variables excluded were mainly specific location tags, and deleting them means this model could be used for judging Eucalypti in other regions, not only the ones coming from New Zealand, where all the data comes from. Our goal was to build an explainable model that achieves results similar to a black box model. In fact, the model constructed in this article uses domain knowledge and application of different preprocessing and machine learning techniques and, as a result, performs better than a black box model. This shows that our goal was achievable and that explainable models can give comparable and sometimes even better results than black box models, in addition to a smaller computational cost and possibility to understand and explain decisions made to solve a problem. Different methods, as in [2] could also be applied in the future, to further improve the model built in this article. 3.1.8 7. References Frank, Eibe &amp; Hall, Mark. (2001). A Simple Approach to Ordinal Classification. Lecture Notes in Computer Science. 2167. 145-156. 10.1007/3-540-44795-4_13 P. A. Gutiérrez, M. Pérez-Ortiz, J. Sánchez-Monedero, F. Fernández-Navarro and C. Hervás-Martínez, “Ordinal Regression Methods: Survey and Experimental Study,” in IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 1, pp. 127-146, 1 Jan. 2016, doi: 10.1109/TKDE.2015.2457911. Gaudette L., Japkowicz N. (2009) Evaluation Methods for Ordinal Classification. In: Gao Y., Japkowicz N. (eds) Advances in Artificial Intelligence. Canadian AI 2009. Lecture Notes in Computer Science, vol 5549. Springer, Berlin, Heidelberg Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell "],
["predicting-code-defects-using-interpretable-static-measures-.html", "3.2 Predicting code defects using interpretable static measures.", " 3.2 Predicting code defects using interpretable static measures. Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology) 3.2.1 Abstract 3.2.2 Introduction and Motivation Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, (1976) (1977)). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program (Fenton and Pfleeger 1997). To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white-box models (e.g. trees, linear regression and k nearest neighbors algorithm). Our goal is to build, using simple data transformations and easily explainable methods, such model that will achieve results comparable to the black-box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated. 3.2.3 Dataset Our dataset comes from the original research of Halstead and McCabe. We obtain it by combining the sets from OpenML (Vanschoren et al. 2013) and supplementing them with data from the PROMISE repository (Sayyad Shirabad and Menzies 2005). It contains data collected from NASA systems written in C and C++ languages. The data is in the form of a data frame containing more than \\(15000\\) records. Each record describes one “program module”. – with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups: Basic measures (such as number of lines of code, number of operands, etc.). McCabe’s measures how complex the code is in terms of control flow and cross-references . Halstead’s measures for general code readability. Target column (1 if module contains defects, 0 if not). Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems). The dataset is slightly imbalanced – about \\(20\\%\\) of records are classified as having defects. In order to verify our hypotheses, we decide at the beginning to remove the Halstead’s measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black-box model without them. We also wanted to remove McCabe’s measurements, but the basic measurements that he used to calculate his measurements are not preserved in the dataset, so we decide to keep them. There are not many records with openly missing data in the set (\\(&lt; 1\\%\\)), however, the values of some columns raise doubts – in the column containing information about the number of lines of code of a given module in many cases there is a value \\(0\\), which is not reliable. However, it turned out during the preliminary analysis that deleting those records significantly worsens the model score, so we decided to keep them. 3.2.4 Methodology Our research consist of the following stages: Data exploration. Initial data preparation. Building of black-box and white-box models and comparing them against the relevant measurements. Repeating the cycle: Improvement of white-box models by modifying their parameters or data. Measuring the effectiveness of the models built. Analysis of the resulting models. Keeping or rejecting changes for further work. Selection of the best white-box model and final comparison with the black-box model. During the step 4. in some cases we decide to take a step back and use other similar transformation or other order of transformations if we suspect that it can yield a better result. We use R programming language and popular machine learning project management packages – mlr (Bischl et al. 2016b) and drake (Landau 2018). 3.2.4.1 Data exploration At this stage, we take a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships. 3.2.4.2 Initial data preparation This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column (in fact, we add five indicator columns, which contain one-hot-encoded value, as models generally do not cope well with character columns). Since there is not much missing data, we impute them with the median, because this method is effective and fast. Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that would still work. We do not carry out further transformations at this stage because we do not want to disturb the results of the next stage. 3.2.4.3 Starting models We build models on this almost unaltered data. We use one poorly interpretable model (black-box) – random forest, specifically ranger package (Wright and Ziegler 2017), because it is fast and low-effort. Among well interpretable models (white-boxes) used in our work there are: logistic regression (lm), decision tree (rpart), k-nearest neighbors algorithm (kknn). We train the models into data that we have divided into five folds with a similar distribution of the decision variable, on which we will perform cross-validation. Then we compare the results using commonly used measure – AUC (Area Under Curve) (Flach, Hernandez-Orallo, and Ferri 2011), which not only assesses whether the observations are well classified, but also takes into account the likelihood of belonging to a class. AUC is not the best measure to be used on imbalanced dataset. However, the unbalance here is not big enough to make this choice unreliable. We use AUC as the main comparative criterion of the models also in the further part of our work. 3.2.4.4 Improving white-boxes This is a key part of our work. In the iterative cycle we use different methods to improve the quality of the white-box models. After applying each of these methods, we check whether it has improved our score and possibly analyze the model, using statistical methods (residuals analysis) and explanatory machine learning (DALEX package (Biecek 2018)), to draw indications of what should be done next. We are trying the following methods: Tuning hyperparameters of models – Default hyperparameters for models are generally good, but in specific cases using specific hyperparameters may yield in better results, so we use model-based optimization for tuning those parameters (Bischl et al. 2017). Reducing outliers – For each variable a two-value vector that indicates the thresholds for which the values are considered as outliers is generated. Then all outliers are changed to the nearest value of obtained earlier vector. Logarithmic and exponential transformations of individual variables – So that linear relationships can be better captured and to reduce the influence of outliers, we transform variables using exponential and polynomial functions. Discretization of continuous features – Some variables do not have a linear effect on the response variable, even if they are transformed by simple functions like exponential function, sometimes there are clear thresholds – so we can replace the variable with indexes of individual segments. The SAFE algorithm helps with this (Gosiewska et al. 2019). Generating new columns as functions of other columns – There may be interactions between variables that cannot be captured by linear models. In order to take them into account, we generate new columns, applying to the rest of them various transformations – we take their inverses, products, quotients, elevations to power, and so on. As a result of these operations, a lot of new measures, potentially simillar to those proposed by McCabe and Halstead, are created, which we later evaluate. We also analyze their interpretability, i.e. to what extent they are translatable into an intuitive understanding of such a measure. At this point we also consider Halstead and McCabe’s measurements. A model with thousands of variables is not well interpretable, so we need to select meaningful measures. We do this by training rpart and ranger models with these additional features and we use DALEXto select the significant ones. We do it twice so at the and we had around 10 of the most important features. Oversampling – On the basis of the data set, we generate more observations from the minority class using the SMOTE algorithm (Hu and Li 2013) so that the model more emphasizes the differences in characteristics of individual classes. In order to avoid model overfitting, we generate data within each fold separately and during crossvalidation we use 4 folds with synthetic data as a train set and the 5th fold without synthetic data as a test set. Our goal is to beat black-box model. In our case we chose random forest model from package ranger. As a white-box model we used logistic regression. Results were tested on dataset with different transformations. 3.2.4.5 Selecting the best model At the end of the process, we select the model that has the highest AUC score for crossvalidation on our dataset. 3.2.5 Results Our base black-box model result is \\(0.792\\). The results of individual models after applying transformations are shown in the Table 1. Order Applied operation logreg kknn rpart Kept? - Base \\(0.735\\) \\(0.728\\) \\(0.500\\) - 0 rpart tuning \\(0.735\\) \\(0.728\\) \\(0.737\\) yes 1a Normalization \\(0.735\\) \\(0.727\\) \\(0.737\\) no 1b Outlier reduction \\(0.743\\) \\(0.732\\) \\(0.739\\) no 1c Logarithm \\(0.744\\) \\(0.718\\) \\(0.725\\) no 2a Outlier reduction and normalization \\(0.743\\) \\(0.732\\) \\(0.739\\) yes 2b Logarithm and outlier reduction \\(0.744\\) \\(0.717\\) \\(0.725\\) no 3a Gain-ratio discretization \\(0.743\\) \\(0.732\\) \\(0.739\\) no 3b rSAFE \\(0.744\\) \\(0.718\\) \\(0.734\\) no 4a New features selected by ranger \\(0.747\\) \\(0.729\\) \\(0.733\\) no 4b New features selected by rpart \\(0.745\\) \\(0.731\\) \\(0.739\\) no 4c Halstead’s measures \\(0.745\\) \\(0.731\\) \\(0.738\\) no 5a SMOTE with new features by ranger \\(0.749\\) \\(0.737\\) \\(0.800\\) no 5b SMOTE with new features by rpart \\(0.747\\) \\(0.736\\) \\(0.793\\) no 5c SMOTE without new features \\(0.745\\) \\(0.736\\) \\(0.804\\) yes Table 1: AUC in white-box models. Each row describes one of the operation that we apply in order to obtain improvement and results of individual models. The first column indicates the order in which we used these transformations. The letters “a, b, c” indicate that we used different transformations with “backtracking”, i.e. we applied them parallel to the same model as they are similar. The last column informs if we decided to keep the change or reject it. Since at the beginning rpart had the AUC value of \\(0.5\\), firstly we tuned its hyperparameters and there was significant improvement in result. Then as step 1a-1c we tried 3 basic transformations of data as normalization of all columns, outlier reduction and logarithm of nearly all columns (without one hot encoded source). As we can see the best improvement was achieved by using outliers reduction so in step 2a we tried to add normalization to that and in step 2b we tried to reduce outliers after applying logarithm. Only outliers reduction and normalization were good enough to be used in the following experiments. Next we used discretization of some columns. Unfortunately, any of the gain-ratio method from funModeling (3a) and rSAFE (3b) made statistically important better result. Using partial dependency plot on the black-box model do not show that there is variable which can be discretized. Then we tried to create new measures based on basic Halstead measures. We selected best measures by DALEX variable importance on one of two learners: ranger or rpart. Then on we test the models after adding the best 10 measures to the dataset, but the results were not promising except from the logistic regression in 4a. Use of the SMOTE algorithm gave a huge improvement especially in decision tree and the best result is in dataset with only outliers reduction and normalization without any new features. 3.2.6 Summary and conclusions The graph shows the improvement of the AUC compared to the difference between the original black-box model and the white-box model. \\(0\\) represents the result of the rpart, \\(100\\) represents the result of the ranger. Since we achieved the main goal and created the interpretable model that is better than black-box the way to achieve that is not satisfying. The most improvement was from just hiperparameters tuning of decision tree and oversampling minority class. The thing we really trusted that can make improvement was the Healstead measures but it do not give any improvement in the model. Also the McCabe’s measures do not add any value to the model. So we can conclude that hypothesis of both of them are wrong. The most informative measure is just a number of code lines. We visualize the best model based on SMOTE. Since we use decision tree and in the last model do not created any new measures so it is interpretable model. It is obvious how it works and why it predicts results, because we can show decision points. Unfortunately this model is not very trivial. Its depth is usually around 6 nodes and in total it have around 50 nodes. That is why we do not include it in the article - it will not be visible. The created plot is on our github. References "],
["using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html", "3.3 Using interpretable Machine Learning models in the Higgs boson detection.", " 3.3 Using interpretable Machine Learning models in the Higgs boson detection. Authors: Mateusz Bakala, Michal Pastuszka, Karol Pysiak (Warsaw University of Technology) 3.3.1 Abstract In this section we compare efficiency of explainable and black-box AI models in detection of the Higgs boson. Afterwards, we explore possible improvements to explainable models which might boost their accuracy to be on par with black-boxes, of which the most notable is R package called rSAFE. Lastly, we conclude that (((conclusion depends on results))). 3.3.2 Introduction and Motivation ‘We fear that, which we do not understand’. This principle has often been used to portray artificial intelligence in popular culture. The notion that it is something beyond human comprehension became so common, that it started to affect the way we perceive computer algorithms. Because of that, it comes as no surprise that people become wary, hearing that the decisions which affect them directly were made by an AI. Machine Learning models, regardless of the algorithm used, are often treated as a black box that takes some data as an input and returns a prediction based on said data. This approach is most often seen from people who are unfamiliar with the methods used. In many fields, where the only concern is achieving results that are as accurate as possible, this is not a concern. There are however situations, where the risk of unpredictable or unjustified results is unacceptable. This includes uses in medical and judicial systems, where an incorrect decision may have severe consequences. Artificial intelligence could find plenty of uses in those areas, but it would require creating models, which make decisions based on a transparent set of rules. Those include algorithms such as decision trees and linear regression. That raises another problem. Such models often underperform in comparison to more advanced algorithms, notably artificial neural networks and ensemble based solutions, which are more capable at detecting nonlinearities and interactions in the data. Being able to achieve results comparable with said models, while retaining explainability would allow to increase the role of machine learning in fields where transparency is required. Furthermore, it would provide tools suitable for scientific analysis of relations between complex phenomena. The problem we study tackles methods of improving the predictions of simple models while retaining their transparency. In the rest of the paper we will describe methods of transforming the data to increase the quality of predictions. Most notably we will focus on a tool named rSAFE, which allows us to extract nonlinearities in the data, based on predictions of a so called ‘surrogate’ model. 3.3.3 Related Work https://arxiv.org/pdf/1402.4735.pdf – Searching for Exotic Particles in High-Energy Physics with Deep Learning 3.3.4 Methodology 3.3.4.1 Data Modern high energy physics is being done in colliders, of which the best known is Large Hadron Collider near Geneva. Here, basic particles are accelerated to collide at almost speed of light, resulting in creation of other, searched for particles. However, not every collision leads to desired processes and even then target particles may be highly unstable. One of such tasks is producing Higgs bosons by colliding two gluons. If a collision is successful, two gluons fuse into electrically-neutral Higgs boson, which then decays into a W boson and electrically-charged Higgs boson, which in turn decays into another W boson and the light Higgs boson. The last one decays mostly into two bottom quarks. However, if a collisions isn’t successful, two gluons create two top quarks, decaying then into a W boson and a bottom quark each, resulting in the same end-products, but without a Higgs boson in an intermediate state. The first process is called ‘signal’, while the second – ‘background’. Our dataset is a dataset called ‘higgs’, retrieved from OpenML database. It contains about 100 thousand rows of data generated with an event generator using Monte Carlo method and is a subset of a HIGGS dataset from UCI Machine Learning Repository. The purpose of this data is to be used as a basis for approximation of a likelihood function, which in turn is used to extract a subspace of high-dimensional experiment data where null hypothesis can be rejected, effectively leading to a discovery of a new particle. Each row describes event, which must satisfy a set of requirements. Namely, exactly one electron or muon must be detected, as well as no less than four jets, every of these with the momentum transverse to the beam direction of value \\(&gt; 20 GeV\\) and an absolute value of pseudorapidity less than \\(2.5\\). Also, at least two of the jets must have b-tag, meaning than they come from bottom quarks. Each event is described by its classification, 1 if a Higgs boson took part in the process or 0 if not. This column is our target, so it’s a two-class classification problem. There are 21 columns describing various low-level features, amongst which there are 16 describing momentum transverse to the beam direction (jetXpt), pseudorapidity (jetXeta), azimuthal angle (jetXphi) and presence of b-tag (jetXb-tag) for each of the four jets, as well as three first of these for the lepton (that is, electron or muon) and missing energy magnitude and its azimuthal angle. There are also 7 columns describing reconstructed invariant mass of particles appearing as non-final products of the collision event. Names like m_jlv or m_wbb mean that the mass reconstructed is of a particle which products consist of jet, lepton and neutrino or a W boson and two bottom quarks respectively. In this case, these particles are namely top quark and electrically-charged Higgs boson. These columns are so-called top-level features. 3.3.4.2 Measures The first measure that we will use is a simple accuracy. It can be misleading, especially when the target variable is unbalanced, but it is very intuitive and in our dataset the target variable is well-balanced. Second measure is the AUC score, which stands for Areas Under Curve of Receiver Operating Characteristic. It requires probabilities as an output of the model. We create the curve by moving the threshold of a minimal probability of a positive prediction from 0 to 1 and calculating the true positive rate to false positive rate ratio. This measure is much less prone to give falsely high scores than accuracy. The worst score in the AUC is \\(0.5\\). We want to get AUC score as close as possible to \\(1\\), however, close to \\(0\\) is not bad, we just must invert the labels and we will get a close to \\(1\\) AUC score. Our last measure is the Area Under the Precision Recall Curve (AUPRC). It is very similar to AUC score, we just use Precision and Recall measures instead of true and false positive rates. It is quite resistant to a bad balance of the target variable. We want to reach the score as close as possible to \\(1\\) and the score of \\(0\\) is the worst possible case. 3.3.4.3 Models The main goal of this research is to find a way of enhancing the performance of interpretable models, so now we will choose algorithms for this task. For the first test algorithm we chose the logistic regression. It is one of the simplest classification methods. Like in the linear regression algorithm we fit a function to the training data, just instead of a linear function we use a logistic function for the logistic regression algorithm. So the interpretability amounts to the understanding of how points are spread throughout the space. The second algorithm that we will use is a decision tree. It’s structure is very intuitive. We start our prediction from the root of a tree. Then we go up to the leaves basing our path on conditions stated in particular nodes. It is similar to how humans make their decisions. If we have to make one big decision we divide it into many smaller decisions which are much easier to make and after answering some yes/no questions we reach the final answer to the question, so to interpret this model we have to understand how these small decisions in every node are made. Black box models usually have very complex structure, which gives them advantage in recognising complex structures of the data. On the other hand, they are very hard to interpret and explain compared to white box models. There is a package in the R language named rSAFE that meets halfway between black box models and white box models. It is a kind of a hybrid that uses black box models for extracting new features from the data and then using them to fit interpretable models. We will use it for enhancing our interpretable model and compare them to pure white box models. To have some perspective of what level of quality of predictions we can reach we will test some black box models. The black box model that we will use is ranger. This is a fast implementation of random forests. This is a light and tunable model, so we will try to reach the best performance as we can with this algorithm. For easily comparable results we will use exactly the same training and testing dataset. However, we do not limit our feature engineering to only one process for all models, because every algorithm can perform better with different features than the rest. What is more, we will focus on augmenting data in a way that will enable us to get as much as we can from simple, interpretable models. 3.3.5 Results 3.3.6 Summary and conclusions "],
["can-automated-regression-beat-linear-model.html", "3.4 Can Automated Regression beat linear model?", " 3.4 Can Automated Regression beat linear model? Authors: Bartłomiej Granat, Szymon Maksymiuk, (Warsaw University of Technology) 3.4.1 Abstract 3.4.2 Introduction and Motivation Health-related problems have been a topic of multiple papers throughout the years and machine learning brought some new methods to modern medicine. We care so much about our lives that every single algorithm and method eventually gets tested on some medical data. What is unique about health data is that black-box models are completely useless in this subject. Almost always doctors know whether a patient is sick or not. What is important to them is the reason why he is sick. That’s why explainable machine learning is the key to make all of us healthier. However, making a good explainable model for health data might be close to impossible. Medical problems of all kinds can be very unique, complex, or completely random. That’s why researchers spend numerous hours on improving their explainable models and that’s why we decided to test our approach on liver disorders dataset. The following dataset is well known in the field of machine learning and that’s exactly the reason why we chose it. It is described in the next chapter. Our goal was to find a relatively clean dataset with many models already done by other people. We don’t want to show that properly cleaned data gives better results but to achieve, an explainable model found after a complex analysis that we want to test. In this paper we do a case study on liver disorders dataset and want to prove that by using automated regression it is possible to build an easy to understand prediction that outperforms black-box models on the real dataset and at the same time achieve similar results to other researchers. 3.4.3 Data The dataset we use to test our hypothesis is a well-known liver-disorders first created by ‘BUPA Medical Research Ltd.’ containing a single male patient as a row. The data consists of 5 features which are the results of blood tests a physician might use to inform diagnosis. There is no ground truth in the data set relating to the presence or absence of a disorder. The target feature is attribute drinks, which are numerical. Some of the researchers tend to split the patients into 2 groups: 0 - patients that drink less than 3 half-pint equivalents of alcoholic beverages per day and 1 - patients that drink more or equal to 3 and focus on a classification problem. All of the features are numerical. The data is available for 345 patients and contains 0 missing values. The dataset consists of 7 attributes: mcv - mean corpuscular volume alkphos - alkaline phosphatase sgpt - alanine aminotransferase sgot - aspartate aminotransferase gammagt - gamma-glutamyl transpeptidase drinks - number of half-pint equivalents of alcoholic beverages drunk per day selector - field created by the BUPA researchers to split the data into train/test sets For further readings on the dataset and misunderstandings related to the selector column incorrectly treated as target refer to: “McDermott &amp; Forsyth 2016, Diagnosing a disorder in a classification benchmark, Pattern Recognition Letters, Volume 73.” 3.4.4 Methodology AutoMl Model \\(M_{aml}\\) and the dataset \\(D\\) that consists of \\(D_{X} = X\\) which is set of independent variables and \\(D_{y} = y\\) - dependent variable (ie. target). We assume that \\(M_{aml}\\) is an unknown function \\(M_{aml}: \\mathbb{R}^{p} \\to \\mathbb{R}\\), where p is a snumber of features in the \\(D\\) Dataset, that satisfies \\(y = M_{aml}(X) + \\epsilon\\) where \\(\\epsilon\\) is an error vector. Automated regression constructs known vector function \\(G_{AR} : \\mathbb{R}^{n \\times p} \\to \\mathbb{R}^{n \\times p}\\) where \\(n\\) is a number of observations, that satisfies \\(y = G_{AR}(X)\\beta + \\epsilon\\) thus it is linear regression model fitted for transformated data. To find \\(G_{AR}\\) we have to put some constraints. First of all we want it to minimize loss function \\(L: \\mathbb{R}^{n} \\to \\mathbb{R}\\) given by following formula \\(L : \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^{2}\\sum_{i=1}^{n}(y_{i}-\\bar{y_{i}})^{2}}{\\sum_{i=1}^{n}(\\hat{y_{i}}-\\bar{y_{i}})^{2}}\\) which can be interpreted as Mean Square Error divided by the R-squred coefficient of determination and stands as a tradeoff between fit and results. Another constraint is a domain of valid transformations of particular variables. For given dataset, described in the previous paragraphs we decided to use: Feature selection XAI feature Importance AIC/BIC Continuous transformation Polynomial transformation Lograthmic transformation Discrete transformation SAFE method Feature concatenation Multiplication of pair of features. Obviously, XAI related methods are conducted using AutoML Model. We’ve decided to omit data imputation as an element of valid transformations dataset because liver-disorders dataset does not meet with the problem of missing values. The optimization process is conducted based on Bayesian Optimization and the backtracing idea. Each main element of the domain of valid transformations is one step in the process of creation \\(G_{AR}\\) function. Within each step, Bayesian optimization will be used to find the best transformation for the given level. During further steps, if any of transformation did not improve model, ie. \\(L\\) function was only growing, the algorithm takes second, the third, etc. solution from previous steps according to backtracking idea. If for no one of \\(k\\) such iterations, where k is known parameter, a better solution is found, step is omitted. 3.4.5 Results 3.4.6 Summary and conclusions "],
["interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html", "3.5 Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.", " 3.5 Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric. Authors: Łukasz Brzozowski, Wojciech Kretowicz, Kacper Siemaszko (Warsaw University of Technology) 3.5.1 Abstract In this article we present and compare a number of interpretable, non-linear feature engineering techniques used to improve linear regression models performance on Cocrete compressive strength dataset. To assert their interpretability, we introduce a new metric for measuring feature importance, which uses derivatives of feature transformations to trace back original features’ impact. As a result, we obtain a thorough comparison of transformation techniques on two black-box models - Random Forest and Support Vector Machine - and three glass-box models - Decision Tree, Elastic Net, and linear regression - with the focus on the linear regression. 3.5.2 Introduction and Motivation Linear regression is one of the simplest and easiest to interpret of the predictive models. While it has already been thorougly analysed over the years, there remain some unsolved questions. One such question is how to transform the data features in order to maximize the model’s effectiveness in predicting the new data. An example of a known and widely used approach is the Box-Cox transformation of the target variable, which allows one to improve the model’s performance with minimal increase in computational complexity (Sakia 1992). However, the choice of the predictive features’ transformations is often left to intuition and trial-and-error approach. In the article, we wish to compare various methods of features’ transformations and compare the resulting models’ performances while also their differences in feature importance. Many black box regression models use various kinds of feature engineering during the training process. Unfortunately, even though the models perform better than the interpretable ones, they do not provide information about the transformations used and non-linear dependencies between variables and the target. The goal we want to achieve is extracting features and non-linearities with understandable transformations of the training dataset. To measure the improvement of used methods we will compare their performance metrics with black box models’ as a ground truth. This will allow us to effectively measure which method brought the simple linear model closer to the black box. Moreover, we will take under consideration the improvement of black box model performance. Thanks to this, our article will not only present the methods for creating highly performant interpretable models, but also improvement of the results of black box model. 3.5.3 Related Work There exist many papers related to feature engineering. We will shortly present two of them. One of these papers is “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering” Patricia Arroba, José L. Risco-Martín, Marina Zapater, José M. Moya &amp; José L. Ayala (Patricia Arroba 2015). This paper describes, how feature transformations in linear regression can be chosen based on the genetic algorithms. Another one is “Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid” Vinícius Veloso de Melo, Wolfgang Banzhaf (Melo] and Banzhaf 2018). Similarly to the previous one, this paper tries to automate feature engineering using evolutionar computation to make a hybrid model - final model is simple linear regression while its features are found by more complex algorithm. 3.5.4 Methodology The main goal of our research is to compare various methods of transforming the data in order to improve the linear regression’s performance. While we do not aim to derive an explicitly best solution to the problem, we wish to compare some known approaches and propose new ones, simultaneously verifying legitimacy of their usage. The second goal of the research is to compare the achieved models’ performances with black box models to generally compare their effectiveness. An important observation about the linear regression model is that once we transform features in order to improve the model’s performance, it strongly affects its interpretability. We therefore propose a new feature importance measure for linear regression and compare it with the usual methodes where possible. 3.5.4.1 Transformation methods The four methods of feature transformation compared in the article include: By-hand-transformations - through a trial-and-error approach we derive feature transformations that allow the linear regression models to yield better results. We use our expertise and experience with previous datasets to get possibly best transformations which are available though such process. They include taking the features to up to third power, taking logarithm, sinus, cosinus or square root of the features. Brute Force method - this method of data transformation generates huge amount of additional features being transformations of the existing features. We allow taking each feature up to the third power, taking sinus and cosinus of each feature and additionaly a product of each pair of features. The resulting dataset has 69 variables including the target (in comparison with 9 variables in the beginning). Bayesian Optimization method (Bernd Bischl 2018) - we treat the task of finding optimal data transformation as an optimization problem. We restrict the number of transformations and their type - we create one additional feature for each feature present in the dataset being its \\(x\\)-th power, where \\(x\\) is calculated through the process of Bayesian optimization. It allows us to keep the dimension of the dataset relatively small while improving the model’s performance. We allowed the exponents to be chosen from interval \\((1, 4]\\). One of our ideas is to use Genetic Programming (GP) to find the best feature transformations. Our goal is to find a set of simple feature transformations (not necessarily unary) that will significantly boost the ordinary linear regression model performance. We will create a set of simple operations such as adding, multiplying, taking a second power, taking logarithm and so on. Each transformation is based only on these operations and specified input variables. Each genetic program tries to minimize MSE of predicting the target, thus trying to save as much information as possible in a single output variable constructed on a subset of all the input features. We will use a variation of the genetic algorithms to create an operation tree minimizing our goal. Before we fit the final model, that is the ordinary linear regression, first we select the variables to transform. The resulting linear regression is calculated on the transformed and selected variables at the very end. To summarize, each operation tree calculates one variable, but this variable may not be included at the end. To avoid strong correlation we decided to use similar trick to the one used in random forest models. Each GP is trained only on some subset of all variables. Bootstraping seems also to be an effective idea, however, we did not implement it. In the case of small number of features in the data, such as in our case, one can consider using all possible subsets of variables of fixed size. Otherwise, they can be chosen on random. This process results in a number of output variables, so we choose the final transformed features using LASSO regression or BIC. This method should find much better solutions without extending dimensionality too much nor generating too complex transformations. The general idea is to automate feature enginnering done traditionally by hand. Another advantage is control of the model’s complexity. We can stimulate how the operation trees are made, e.g. how the operations set looks like. There is also a possibilty of reducing or increasing complexity at will. Modification of this idea is to add a regularization term decreasing survival probability with increasing complexity. At the end, the model could also make a feature selection in the same way - then one of possible operations in the set would be dropping. 3.5.4.2 Feature importance metric The most natural feature importance metric for linear models such as linear regression and GLM are the absolute values of coefficients. Let \\(x_1, \\dots, x_n\\) denote the features (column vectors) and \\(\\hat{y}\\) denote the prediction. In linear models we have: \\[c + \\sum_{i=1}^n a_i \\cdot x_i = \\hat{y}\\] where \\(c\\) is a constant (intercept) and \\(a_i\\) are the coefficients of regression. Formally, the Feature Importance mesaure value of the \\(i-\\)th feature (\\(FI_i\\)) measure is given as: \\[FI_i = |a_i|\\] We may also notice that: \\[FI_i = \\Big|\\frac{\\partial \\hat{y_i}}{\\partial x_i}\\Big|\\] We wish to generalize the above equation. Transforming the data in the dataset to improve the model’s performance may be expressed as generating a new dataset, where each column is a function of all the features present in the original dataset. If the newdataset has \\(m\\) features and the new prediction vector is given as \\(\\dot{y}\\), we then have: \\[d + \\sum_{i=1}^m b_i \\cdot f_i (x_1, \\dots, x_n) = \\dot{y}\\] where \\(d\\) is the new intercept constant and \\(b_i\\) are the coefficients of regression. We therefore may use the formula above to derive a new Feature Importance Measure (Derivative Feature Importance, \\(DFI\\)) as: \\[DFI_{i, j} = \\Big|\\frac{\\partial \\dot{y_i}}{\\partial x_{i, j}}\\Big| = \\Big|b_i \\cdot \\sum_{k=1}^m \\frac{\\partial f_k}{\\partial x_{i, j}}\\Big|\\] The above measure is calculated separately for each observation \\(j\\) in the dataset. However, due to the additive properties of derivatives, we may calculated Derivative Feature Importance of the \\(i\\)-th feature as: \\[DFI_i = \\frac{1}{n}\\sum_{j=1}^n DFI_{i, j}\\] which is then a global measure of Feature Importance of the \\(i\\)-th feature. 3.5.4.3 Dataset and model performance evaluation The research is conducted on Concrete_Data dataset from the OpenML database [link]. The data describes the amount of ingredients in the samples - cement, blast furnace slag, fly ash, water, coarse aggregate and fine aggregate - in kilograms per cubic meter; it also contains the drying time of the samples in days, referred to as age. The target variable of the dataset is the compressive strength of each sample in megapascals (MPa), therefore rendering the task to be regressive. The dataset contains 1030 instances with no missing values. There are also no symbolic features, as we aim to investigate continuous transformations of the data. Due to the fact, that we focus on the linear regression model, the data is reduced prior to training. We remove the outliers and influential observarions based on Cook’s distances and standardized residuals. We use standard and verified methods to compare results of the models. As the target variable is continuous, we may calculate Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared measures for each model, which provide us with proper and measurable way to compare the models’ performances. The same measures may be applied to black box models. The most natural measure of feature importance for linear regression are the coefficients’ absolute values after training the model - however, such easily interpretable measures are not available for black-box models. We therefore measure their feature importance with permutational feature importance measure and caluclating drop-out loss, easily applicable to any predictive model and therefore not constraining us to choose from a restricted set. In order to provide unbiased results, we calculate the measures’ values during cross-validation process for each model, using various number of fold to present comparative results. 3.5.5 Results Note, that the transformations used in results section are only examples of the presented methods and various realisations may result in various final measures’ values. Considerably, the trial-and-error approach may is quite individual and the results may differ depending on the experimentator. This non-automatic method should be treated as something that can be achieved after long time of tries. 3.5.5.1 Models’ performance FIGURE 3.1: MAE of the models after transformations FIGURE 3.2: MSE of the models after transformations The plots presented above show the values of MAE and MSE achieved by each model on the datasets after the mentioned transformations. We may observe that the linear models have significantly reduced their error values after the transformations, while the brute force method yielded best results. However, brute force method generates much more features increasing the resulting dimension of the dataset, thus increasing the time complexity and reducing the interpretability. The three remaining methods, that is: the Bayesian optimization, the trial-and-error method and the genetic modifications - provided much improvement in comparison with the models’ performance on the plain dataset as well. In this case the final space has much lower dimension. Bayesian optimization results in 16 features, the trial-and-error in 14 features and the genetic modifications result in 8 features, in comparison to 69 features after brute force transformations. All of these methods have very similar quality of predictions, considering both MAE and MSE. The remaining non-linear white-box model, namely the Decision Tree Regressor, seems to be rather unaffected by any transformations of the dataset. In comparison, both black the boxes: Random Forest and SVM with gaussian kernel, are strongly influenced, though is hard to say when black box’s prediction quality increases and when decreases. 3.5.5.2 Feature Importance comparison The comparison of Feature Imprortance values will be between permutational Feature Importance calculated on the black-box models - Random Forest and SVM and local and global Derivative Feature Importance calculated on the datasets after brute force transformations and after the Bayesian optimization. FIGURE 3.3: Feature Importance of RFR and SVM FIGURE 3.4: Local and global DFI on the dataset after brute force transformations FIGURE 3.5: Local and global DFI on the dataset after Bayesian transformations The figure 3.4 present permutational Feature Importance of the black-box models, and the figures 3.5 and 3.6 present \\(DFI\\) values - for one observation and calculated globally, respectively. The local \\(DFI\\) were calculated on the observations nr \\(573\\) and \\(458\\). We may notice high resemblance of the black-box Feature Importance to the local \\(DFI\\) after Brute Force trtansformations, as well as similarities between all the Feature Importance calculations overall. The order of features in \\(DFI\\) is not precisely equal to the order of feature after permutational Feature Importance of the black-boxes; however, the order still makes sense as far as we can say based on our little knowledge of materials. Moreover, the deviations of Feature Importance measures between various models is quite common in such research. To summarize, although the deviations between black-box and glass-box models’ Feature Importance are present, we conclude that \\(DFI\\) may provide a new way of calculating Feature Importance for linear models. Its efficiency shall be further investigated in another research. When it comes the presented comparison, the majority of important variables were detected by the \\(DFI\\) method. 3.5.6 Summary and conclusions Each of the four methods leads to significant improvement in Linear Regression. All of them are based on completely different ideas and their effectivenes may vary for different tasks. It needs noting that the presented research is conducted on a dataset containing only numerical variables, so similar research for transformations of categorical variables remains to be yet conducted. However, we have presented numerous ways to transform the dataset improving the linear models while at least partially maintaining its interpretability. The black box models in this case were unsurpassed, though we achieved highly comparable results. The greatest advatange of white box usage is that every person can understand their mechanics and predictions. Therefore, the presented methods may serve as an efficient solution, when one needs to retain simplicity while also offering an improvement to predictions. Our new metric - \\(DFI\\) - can be used to compare simple (differentiable) feature transformations in linear regression. The analytical deduction suggests, that its performance is accurate, but it shall be investigated further before being applied in general. In the end, the obtained results are satisfying and should encourage to put more effort into research about new transformation methods and interpretability metrics. The next thing to do is putting more effort into researching the new \\(DFI\\) metric, to improve interpretability of the extended regression models. We hope that our article will inspire a number of interested readers to conduct such research. References "],
["surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html", "3.6 Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering", " 3.6 Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering Authors: Witold Merkel, Adam Rydelek, Michał Stawikowski (Warsaw University of Technology) 3.6.1 Abstract Explainability is the most talked about topic of modern predictive models. The article touches on the topic of such models and their benefits. The main focus is to prove that even on complicated data, explainable models can achieve comparable performance to the best black-box models. Not only are there described strategies allowing better results but also greater explainability. The dataset used in experiments is the adult dataset from OpenML which is from Census database. During the experiments there are multiple processing techniques used, SAFE and different imputation methods among others. Every tool used is explained and the results gained from each part are shown and explained. Thanks to the fact that adult dataset is vastly unbalanced there is a perfect opportunity to present techniques which can be used to handle such tasks. All those methods combined allow for a presentation of a clear workflow enhancing explainable models performance with emphasis on decision tree models. The best results we achieved with decision tree model using methods mentioned above. However at first the best score was achieved by logistic regression, which from the start beat the black boxes. On the other hand it was not possible to tune it, to get it any better. For this reason, our final model is a decision tree, that despite starting as one of the worst surpasses all of the other boxes white and black. This shows that everything can be accomplished with adequate feature engineering, while keeping them explainable. 3.6.2 Introduction and Motivation Recently, an increase in demand of interpretable models can be seen. Machine learning models have gained in popularity in recent years among many fields of business science, industry and also more and more often in medicine. Interpretability is a quickly growing part of machine learning, and there have been many works examining all of the aspects of interpretations. (Murdoch 2018) The problem, however, turned out to be blackbox models, which did not provide sufficient information about the motivation in making specific decisions by the models. Some of machine learning models are considered as black boxes. This means that we can get accurate predictions from them, but we give up the ability of clearly explaining or identifying the logic behind these decisions. (Pandey 2019) Interpretability of models is a desirable feature among specialists in fields other than machine learning, it helps them make better decisions, justify their choices, and combine expert knowledge with the model’s indications. Humans and computers work differently in how they sense, understand and learn. Machines deals with large volume of data and finding hidden patterns in it and people are better at seeing the bigger picture and finding high-level patterns. (accenture 2018) Trust and transparency are also demanded. There are many methods that can help us create an interpretable model. One of the ways to achieve interpretability is to use only a certain subset of algorithms that create interpretable models. Some of the algorithms considered to be interpretable are: linear regression, logistic regression, decision trees or K Nearest Neighbours (KNN). (Molnar 2019) Another way may be to use blackboxes to create an interpretable model. They can help us during transformation of the original data set or, for example, in selecting variables. In this article, we will discuss the process of creating an interpretable model whose target effectiveness will be comparable to blackbox models. We will present the whole workflow, during which we will get acquainted with the dataset with which we will work, we will use advanced feature engineering methods and compare the results obtained during all phases of process. An additional problem we will face during work will be unbalanced data and creating a model that will take them into account during prediction. We will use machine learning tools and frameworks available in R and Python. 3.6.3 Data The dataset used is the adult dataset from OpenML. The original data comes from UCI and was extracted by Barry Becker from the 1994 Census database. The task is to predict whether a given adult makes more than $50,000 a year based attributes such as: age, race, sex, education, native country, work class, weekly work hours, capital gain, capital loss, proximation for the demographic background of people, relationship, marital status, occupation. In the above mentioned dataset we can observe a problem with target class distribution which is vastly unbalanced. The ratio of positive and negative values is around one to four. The dataset has overall of more than forty eight thousand observations and fifteen features, some of which are scarce. 3.6.4 Related work Many works concerning Explainable Artificial Intelligence have arose during the last few years as the topic got more and more popular. [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI] (Alejandro Barredo Arrieta 2019) is a paper about XAI in general and many challenges concerning the topic. The article addresses all kinds of easily explainable models which set our focus on enhancing kNN and decision tree based models. [SAFE ML: Surrogate Assisted Feature Extraction For Model Learing] (Gosiewska et al. 2019) on the hand focuses on using Black Box models as surrogate models for improving explainable models. 3.6.5 Methodology As mentioned before we are going to work on an unbalanced dataset. In order to handle this issue and achieve the best possible results during our future work we are going to use two measures: AUPRC and AUC. The former one is designed to take the lack of balance into account. The dataset will be divided into two partitions using stratification in order to handle scarce factor levels. The training part of the dataset is going to be used to compare the effects of many processes used to enhance the results. We are going to use five fold cross-validation. The final results are going to be presented using the test dataset. Our workflow can be divided into the following steps: 3.6.5.1 EDA In this part we have gotten accustomed with the dataset. We started with feature distribution and definition analysis and studied the dependency of other variables on the target class. The column correlation was also taken into account. 3.6.5.1.1 Distribution of numeric variables Data distribution On this plot we can see the distribution of numeric variables. Some of them are normally distributed but there are also highly skewed variables such as capital-gain and capital-loss. 3.6.5.1.2 Correlation plot of numeric variables Correlation On this plot we can observe that the numeric variables have weak correlations with each other. 3.6.5.1.3 Density of observations by age and education Density We can see the density of observations divided by age and education-num features. Education-num is a numeric variable that depicts the education level in an ordered way. 3.6.5.1.4 Level of education by target class Education We can observe that the highest rate of positive target class is for PHD and professors which does not surprise. 3.6.5.1.5 Work class by target class Work class The highest rate when divided by work class can be observed for self-employed people. 3.6.5.1.6 Age by target class Age The density of people earning more than $50000 is skewed toward the elderly. 3.6.5.2 Initial Data Preparation The main focus of this part is to analyze and input the missing data. The side tasks are handling outliers and transformation of skewed variables using logistic functions. A few most popular imputation methods will be compared. We are not only going to use basic aggregation functions like mean and mode but also other machine learning models and advance imputation methods like (Stef van Buuren 2011). The results are going to be compared using a model that is robust to missing data which is a basic decision tree. 3.6.5.2.1 Missing Data by Feature Missing Data Missing data can be observed in three columns. They account up to eight percent of data. 3.6.5.2.2 Missing Data Pattern Missing Data Pattern The bottom figure shows relation between missing variables, when a certain variables is absent in correspondence to other. 3.6.5.2.3 Imputation Results Imputation type AUPRC AUC none 0.6457237 0.8357408 Mean and mode basic imputation 0.6472785 0.8362379 KNN imputation 0.6547221 0.838437 MICE imputation 0.6452505 0.834577 Missing data removal 0.6515376 0.8355305 We can observe that the best results were achieved using KNN imputation so from now on this will be the used method. 3.6.5.2.4 Outliers Missing Data Some outliers can be observed in fnlwgt columns which is described as proximation for the demographic background of people. The data was cropped to 300000 value. 3.6.5.2.5 Skewness Skewness of fnlwgt The previously mentioned feature was also skewed. The solution for this problem was a basic logistic transformation. Transformation of fnlwgt The transformation was successful and the new feature has an appropriate distribution. 3.6.5.3 Feature Engineering and Tuning Firstly we are going to compare a few most popular Machine Learning models on our initially prepared dataset. We picked three popular explainable models: KNN, Decision Tree and Logistic Regression. 3.6.5.3.1 First comparison of models First results The best result was achieved by logistic regression which was surprising and shows how capable explainable models can be even on complex data, non of later modifications had any impact on the model so we decided to exclude it from further considerations. The next best explainable model ranked by AUPRC was KNN and Decision Tree. Due to the fact that logistic regression obtained a better result than black boxes at the very start, and the KNN model is not globally interpretable, we will focus on the decision tree, which is a popular and easily interpretable model. That is the reason why we are going to work mainly with this model and initially with KNN for comparison in our goal to achieve similar results to Black Box models using it, but our final model will be a decision tree. The best Black boxes were Random Forest and Adaboost, because of that in the later phase of our project we will depend on them to make our results better. During the Feature Engineering we will utilize strategies such as transforming and extracting features using the SAFE algorithm mentioned in the article above. Amongst other strategies we will use variable selection based on random forest feature importance and tuning models with Bayesian optimization - mlrMBO (Bernd Bischl 2018) based on custom measure: AUPRC and not the typical AUC. Another interesting aspect that we are going to look into is changing the type of target to numeric and by doing so changing our task from classification to regression, surprisingly it improves our results. 3.6.5.3.2 rSAFE The SAFE ML algorithm uses a complex model as a surrogate. New binary features are created on the basis of surrogate predictions. These new features are used to train a simple refined model. (…) method that uses elastic black-boxes as surrogate models to create a simpler, less opaque, yet still accurate and interpretable glass-box models. New models are created on newly engineered features extracted/learned with the help of a surrogate model. (Gosiewska et al. 2019) To extract new variables we will use rSAFE, which will depend on two different Black Box models: Random Forest and Adaboost. Those models were tuned to increase the final result. Based on the initial variables and the new ones created by the rSafe algorithm, we will choose the combination that will be the best for this problem. We will achieve this by analyzing various values of the “regularization penalty” parameter in the data extraction algorithm in the rSafe package and choosing the one that gives us the best result. Model From AUPRC To AUPRC From AUC To AUC decision tree 0.655 0.674 0.842 0.851 knn 0.698 0.706 0.859 0.868 From the above table it can be observed that this method gave us a significant improvement in terms of AUC and AUPRC. New variables that were extracted and chosen with penalty regularization are: “age_new” - new levels: (-Inf, 32] and (32, Inf), “fnlwgt_new” - new levels: (-Inf, 11.70863] and (11.70863, Inf), \"hours.per.week_new - new levels: (-Inf, 40] and (40, Inf), “race_new” - new levels: White_Asian-Pac-Islander and Amer-Indian-Eskimo_Other_Black. 3.6.5.3.3 Change to regression The next approach we chose based on our previous experience with unbalanced datasets is changing our task from classification to regression and by doing so we improved our prediction. Non of later modifications had any impact on the KNN model so we decided to exclude it from further considerations. Model From AUPRC To AUPRC From AUC To AUC decision tree 0.674 0.716 0.851 0.865 In case of KNN there was no visible improvement. 3.6.5.3.4 Variable selection Additionally we carried out a variable selection on those that are currently used. It was based on feature importance from a random forest model, we use ten most important variables from fifteen that were being used before. Model From AUPRC To AUPRC From AUC To AUC decision tree 0.716 0.720 0.865 0.871 The improvement is slight, but because of this operation we are using a less complicated model, so we decided to use it. 3.6.5.3.5 Tuning Last phase was tuning our decision tree model. We based it on mlrMBO, but our method is adjusted to the task. The optimized measure is AUPRC, this proces is slightly different from optimizing AUC. Before using MBO we select the optimal range of parameters like: “minsplit” - the minimum number of observations that must exist in a node in order for a split to be attempted. and “minbucket” - the minimum number of observations in any terminal node.. After selecting ten optimal values of the first one we select ten optimal for the second. For each of those hundred pairs we use MBO to find the optimal “cp” - complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. The result we achieved: Model From AUPRC To AUPRC From AUC To AUC decision tree 0.720 0.794 0.871 0.912 Results achieved in this phase are really high, we managed to top those from black boxes and the logistic regression one. 3.6.5.4 Results and conclusion Differences between each stage From the above chart, we can see that thanks to our actions the performance of a fully interpretable model surpassed all basic black boxes and logistic regression, which from the start had really high results. It proves that it is possible to achieve comparable or even better results with interpretable models than black boxes even on unbalanced datasets, such as “adult”. It requires some work, but there are a lot of methods that make it possible to improve our performance. Some of them may be based on very well-functioning black box models. Probably in comparison with tuned black box model, we would achieve lesser results, but interpretable models have an advantage, people can understand why some choices were made and can be safely use when transparency and ease of understanding are needed. Using models such as logistic regression or decision trees make it possible. In this paper we showed that models built using these algorithms after some modifications are able to get similar results to such powerful models as random forest or adaboost, and even artificial deep neural networks. References "],
["which-neighbours-affected-house-prices-in-the-90s.html", "3.7 Which Neighbours Affected House Prices in the ’90s?", " 3.7 Which Neighbours Affected House Prices in the ’90s? Authors: Hubert Baniecki, Mateusz Polakowski (Warsaw University of Technology) 3.7.1 Abstract The house price estimation task has a long-lasting history in economics and statistics. Nowadays, both worlds unite to exploit the machine learning approach; thus, achieve the best predictive results. In the literature, there are myriad of works discussing the performance-interpretability tradeoff apparent in the modelling of real estate values. In this paper, we propose a solution to this problem, which is a highly interpretable stacked model that outperforms the black-box models. We use it to examine neighbourhood parameters affecting the median house price of the United States regions in 1990. 3.7.2 Introduction Real estate value varies over numerous factors. These may be obvious like location or interior design, but also less apparent like the ethnicity and age of neighbours. Therefore, property price estimation is a demanding job that often requires a lot of experience and market knowledge. Is or was, because nowadays, Artificial Intelligence (AI) surpasses humans in this task (Conway 2018). Interested parties more often use tools like supervised Machine Learning (ML) models to precisely evaluate the property value and gain a competitive advantage (Park and Bae 2015, @3–7–realestate–ml2, @3–7–realestate–ml3). The dilemma is in blindly trusting the prediction given by so-called black-box models. These are ML algorithms that take loads of various real estate data as input and return a house price estimation without giving their reasoning. Black-box complex nature is its biggest strength and weakness at the same time. This trait regularly entails high effectiveness but does not allow for interpretation of model outputs (Baldominos et al. 2018). Because of that, specialists interested in supporting their work with automated ML decision-making are more eager to use white-box models like linear regression or decision trees (Selim 2009). These do not achieve state-of-the-art performance efficiently, but instead, provide valuable information about the relationships present in data through model interpretation. For many years houses have been popular properties; thus, they are of particular interest for ordinary people. What exact influence had the demographic characteristics of the house neighbourhood on its price in the ’90s? Although in the absence of current technology, it has been hard to answer such question years ago (Din, Hoesli, and Bender 2001), now we can. In this paper, we perform a case study on the actual United States Census data from 1990 (???) and deliver an interpretable white-box model that estimates the median house price by the region. We present multiple approaches to this problem and choose the best model, which achieves similar performance to complex black-boxes. Finally, using its interpretable nature, we answer various questions that give a new life to this historical data. 3.7.3 Related Work The use of ML in the real estate domain is a well-documented ground (Conway 2018) and not precisely a topic of this contribution. We relate to the works that aim to use Interpretable ML techniques (???) to interpret models predictions in the house price estimation problem. The state-of-the-art approach to house price estimation is to combine linear and semi-log regression models with the Hedonic Pricing Method (Garrod and Willis 1992, @3–7–hpm2), which aims to determine the extent that environmental or ecosystem factors affect the price of a good. (Özalp and Akinci 2017) deliberately seeks to interpret the outcome and provide information about the parameters that affect property value. There are also comparisons between the linear white-box and black-box models (Selim 2009, @3–7–multiple–models) which showcase the performance-interpretability tradeoff (Gosiewska and Biecek 2020). Nature of the topic might entail that the data is place-specific; therefore, part of the studies focus on a single location with the use of geospatial data. The case study on London (Law 2017) links the street network community structure with house price, which takes into consideration the topology of the city. In contradiction, (Heyman and Sommervoll 2019) uses the Oslo city data to explore the differences between the relative and absolute location attributes. Applying the data like the distance to the nearest shop and transportation is place-agnostic. One of the new ideas is to utilize the location data from multiple sources in a Multi-Task Learning approach (Gao et al. 2019). It also studies the relationships between the tasks, which gives an extensive insight on prediction attributions. In this paper we partialy aim to enhance the use of regression decision tree models, which had been utilized to estimate the house prices based on their essential characteristics in (Fan, Ong, and Koh 2006). 3.7.4 Data For this case study we use the house_8L dataset crafted from the data collected in 1990 by the United States Census Bureau. Each record stands for a distinct United States state while the target value is a median house price in a given region. The variables are presented in Table 3.1. TABLE 3.1: Description of variables present in the house_8L dataset. Original name New name Description price price median price of the house in the region P3 house_n total number of households H15.1 avg_room_n average number of rooms in an owner-occupied Housing Units H5.2 forsale_h_pct percentage of vacant Housing Units which are for sale only H40.4 forsale_6mplus_h_pct percentage of vacant-for-sale Housing Units vacant more then 6 months P11.3 age_25_64_pct percentage of people between 25-64 years of age P16.2 family_2plus_h_pct percentage of households with 2 or more persons which are family households P19.2 black_h_pct percentage of households with black Householder P6.4 asian_p_pct percentage of people which are of Asian or Pacific Islander race Furthermore, we will apply our Metodology (Section 4) on a corresponding house_16H dataset, which has the same target but a different set of variables. More correlated variables of a higher variance make it significantly harder to estimate the median house price in a given region. Such validation will allow us to evaluate our model on a more demanding task. The comprehensive description of used data can be found in (“Census dataset” 1996). 3.7.5 Methodology In this section, we are going to focus on developing the best white-box model, which provides interpretability of features. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we focus on the residuals while the mean of absolute values of residuals is the easiest to interpret. 3.7.5.1 Exploratory Data Analysis The main conclusions from the Exploratory Data Analysis are as follows: The target value is very skewed (See Figure 3.6). There are 6 percentage and 2 count variables. The dataset has over 22k data points. There are 46 data points with unnaturally looking target value. There are no missing values. FIGURE 3.6: (L) Histogram of the target values. (R) Examplary variable correlation with the target. Therefore we decided that: We will not transform the skewed target because this might provide less interpretability. There are not many possibilities for feature engineering. We can reliably split the data into train and test using 2:1 ratio. We suspect that the target value of 500001 is artificially made, so we remove these outliers. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we later focus on the residuals while the mean of absolute values of residuals is the easiest to interpret. 3.7.5.2 SAFE The first approach was using the SAFE (Gosiewska et al. 2019) technique to engineer new features and produce a linear regression model. We trained a well-performing black-box ranger (Wright and Ziegler 2017) model and extracted new interpretable features using its Partial Dependence Profiles (Friedman 2000). Then we used these features to craft a new linear model which indeed was better than the baseline linear model by about 10%. It is worth noting that both of these linear models had a hard time succeeding because of the target skewness. 3.7.5.3 Divide-and-conquer In this section, we present the main contribution of this paper. The divide-and-conquer idea has many computer science applications, e.g. in sorting algorithms, natural language processing, or parallel computing. We decided to make use of its core principles in constructing the method for fitting the enhanced white-box model. The final result is multiple tree models combined which decisions are easily interpretable. The proposed algorithm presented in Figure 3.7 is: Divide the target variable with k middle points into k+1 groups. Fit a black-box classifier on train data which predicts the belonging to the i-th group. Use this classifier to divide the train and test data into k+1 train and test subsets. For every i-th subset fit a white-box estimator of target variable on the i-th train data. Use the i-th estimator to predict the outcome of the i-th test data. FIGURE 3.7: The divide-and-conquer algorithm used to construct an enhanced white-box model. (1) Divide the target variable with k middle points into k+1 groups. (2) Fit a black-box classifier on train data which predicts the belonging to the i-th group. (3) Use this classifier to divide the train and test data into k+1 train and test subsets. (4) For every i-th subset fit a white-box estimator. The final product is a stacked model with one classifier and k+1 estimators. The exact models are for engineers to choose. It is worth noting that the unsupervised clustering method might be used instead of the classification model. 3.7.6 Results 3.7.6.1 The stacked model For the house price task, we chose k = 1, and the middle point was arbitrary chosen as 100k, which divides the data into two groups in about a 10:1 ratio. We used the ranger random forest model as a black-box classifier and the rpart (Therneau and Atkinson 2019) decision tree model as a white-box estimator. The ranger model had default parameters with mtry=3. The parameters of rpart models were: maxdepth = 4 - low depth reassures the interpretability of the model cp = 0.001 - lower complexity helps with the skewed target minbucket = 1% of the training data - more filled tree leaves adds up to higher interpretability Figure 3.8 depicts the tree that estimates cheaper houses, while Figure 3.9 presents the tree that estimates more expensive houses. FIGURE 3.8: The tree that estimates cheaper houses. Part of the stacked model. FIGURE 3.9: The tree that estimates more expensive houses. Part of the stacked model. Interpreting the stacked model presented in Figures 3.8 &amp; 3.9 leads to multiple conclusions. Firstly, we can observe the noticeable impact of features like the total number of households or the average number of rooms on the median price of the house in the region, which is compliant with basic intuitions. It is also evident that the bigger percentage of people between 25-64 years of age the higher the prices. Finally, we can observe the impact of critical features. The percentage of people which are of Asian or Pacific Islander race divides the prices in an opposing direction to the percentage of households with black Householder. The corresponding tree splits showcase which neighbours, and in what manner, affected house prices in the ’90s. Whether it is a correlation or causality is a valid debate that could be further investigated. 3.7.6.2 Comparison of the residuals In this section, we compare our stacked model with baseline ranger and rpart models, respectively referred to as black-box and white-box. Our solution achieves competitive performance with interpretable features. The main idea behind the divide-and-conquer technique was to minimize the maximum value of the absolute residuals, which reassures that no significant errors will happen. Such an approach may inevitably lead to minimizing the sum of the absolute residual values (MAE). In Figure (fig:3-7-boxplot), we can see that the targets mentioned above were indeed met. The stacked model not only has the lowest maximum error value but also has the best performance on average, as the red dot highlights the MAE score. FIGURE 3.10: Boxplots of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses. The red dot highlights the MAE score. Figure (fig:3-7-density) presents a more in-depth analysis of model residuals. In the top, we can observe that the black-box model has the lowest absolute residual mode (tip of the distribution), but the stacked model lays more in the centre (base of the distribution), which leads to more even spread of residuals. In the bottom, we can observe that the black-box model tends to undervalue house prices, while our model overestimates them. Looking at the height of the tip of the distribution and its shape, we can conclude that the stacked model provides more reliable estimations. FIGURE 3.11: Density of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses. 3.7.6.3 Comparison of the scores Finally, we present the comparison of MAE scores for all of the used models in this case study in Table 3.2. There are two tasks with different variables, complexity and correlations. We calculate the scores on the test subsets. We can see that the linear models performed the worse, although the SAFE approach noticeably lowered the MAE. Then there is a decision tree which performed better but not so on the more laborious task. Both of the black-box models did a far better job at house price estimation than interpretable models. Finally, our stacked model is a champion with the best performance on both of the tasks. TABLE 3.2: Comparison of the MAE score for all of the used models on test datasets. Green colour highlights white-box models, while the red colour is for black-box models. Dataset (test) Model house_8L house_16H linear model 23.1k 24.1k SAFE on ranger 21.4k 22.6k rpart 19.2k 22.1k xgboost 16k 16.5k ranger 14.8k 15.6k stacked model 14.6k 15.3k 3.7.7 Conclusions TODO: Conclusions References "],
["explainable-computer-vision-with-embeddings-and-knn-classifier.html", "3.8 Explainable Computer Vision with embeddings and KNN classifier", " 3.8 Explainable Computer Vision with embeddings and KNN classifier Authors: Olaf Werner, Bogdan Jastrzębski (Warsaw University of Technology) 3.8.1 Abstract 3.8.2 3.8.1 Introduction Computer vision is widely known use case for neural networks. However neural networks are infamous for their complexity and lack of interpretability. On the other hand simple classifiers like KNN have really poor results for complex tasks like image recognition. In this article we will prove that it is possible to get best of both worlds using emmbeddings. 3.8.3 3.8.2 Data We are going to use dataset Fashion-Mnist. Fashion-MNIST is a dataset of Zalando’s article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Classes are following: T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot. 3.8.4 3.8.3 Methodology The simplest and one of the most robust classifiers is KNN. It doesn’t generalize information, instead it saves training dataset and during prediction it finds the most similar historical observations and predicts label of a new observation based on their labels. However, it not only doesn’t have the capacity to distinguish important features from not important ones, but also to find more complex interactions between variables. One way to improve KNN’s performance is to preced closest neighbour computation with transformation of the space of observations, so that the derivative variables are more meaningful. Such beneficial transformation is called embedding. It can be done in various ways. The question is, whether or not the new classifier is interpretable. We argue, that it is. The main reason is that even if we can’t interpret the embedding part, we can at least provide historical data that our model used to make prediction. Someone could argue, that it can be done with every classifier, just by finding training data that obtains the most similar prediction. However, with our classifier we can say for sure, that prediction were purely made based on the most similar cases in our dataset, which is fundamentally not true about different classifiers. An embedding can be done made in various ways. In this article we will explore different embedding techniques, including: SVD embedding Convolutional Autoencoder K-means embedding 3.8.5 3.8.4 Standard Intepretable Models In this section we will explore use of standard interpretable models and we will try to answer the question, why they are not useful when it comes to computer vision. 3.8.5.1 3.8.4.1 Logistic Regression Logistic regression is basic classification model. We get probablity of belonging to given class by: \\[{\\displaystyle p={\\frac {e^{\\beta _{0}+\\beta _{1}x_{1}+...+\\beta _{n}x_{n}}}{e^{\\beta _{0}+\\beta _{1}x_{1}+...+\\beta _{n}x_{n}}+1}}}\\] where \\(\\beta _{0},\\beta _{1},...,\\beta _{n}\\) are coefficients of logistic regression. We obtain coefficients using gradient descent. Because we have multiple labels so we train 10 diffrent logistic regression models and use softmax function to normalize probablities of belonging to any particular class. We then visualize coeficients as images with bright spots indicating that they are importent. Unfortunately the results look like this: fig. 1: An example of logistic regression weights We get this because logistic regression works more like a sieve. 3.8.5.2 3.8.4.2 Decision Trees Decision trees are very useful interpretable classifiers, however they are suitable when we have very few meaningful dimensions. A tree that does splits based on particular pixels is not a good classifier and it’s explanation provides little knowledge about, why those particular pixels has been chosen. That’s why we will not explore the use of this class of classifiers. 3.8.6 3.8.5 Our Approach In this section we will show alternative to logistic regression and decision trees, that is more interpretable and in the same time has a capacity to obtain significantly better results. 3.8.6.1 3.8.5.1 The KNN Classifier KNN (k nearest neighbours) is a classifier, that doesn’t generalize data. Instead, we keep the training dataset and every time we make a prediction, we calculate distance (for instance euclidean distance) between our new observation and all observations in the training dataset to find k nearest. Prediction is based on their labels. KNN is a robust classifier, that copes with highly non linear data. It’s also interpretable, because we can always show k nearest neighbours, which are an explanation by themselves. It, however, is not flawless. It for instance poorly scales with the size of the training dataset, while in the same time it need, at least in some domains, very big training dataset, as it doesn’t generalize any information. We can significantly improve it’s performance by introducing complex similarity functions. If similarity function is interpretable, we obtain highly interpretable classifier. If not, we get semi interpretable classifier, where we cannot tell, why obsevations are similar according to the model, however we can at least show similar training set examples, based on which prediction has been made. This complex distance functions can be made in many different ways. In this paper we explore functions of a form: \\[distance(Img1, Img2) = d_e(Embedder(Img1), Embedder(Img2))\\] where \\(d_e\\) is euclidean distance, so we simply compute euclidean distance between embeddings of images. Here’s a scheme of KNN classifier: As we can see on fig. n, new image firstly gets embedded and then a standard classification with KNN is made. This type of architecture allowes us to create robust and interpretable classifier. 3.8.6.2 3.8.5.2 Embedding techniques In this section we will explore different embedding techniques. 3.8.6.2.1 3.8.5.2.1 K-means Our problem is supervised one, but we can still use unsupervised aproach to get better results. We use K-means algorithm (also known as Lloyd’s algorithm) to find subclasses in every class. Algorithm: Initiate number of random centroids For every observation find nearest centroid Calculate average of observations in every group found in point 2 This averages becomes new centroids Repeat points 2 to 4 until all new centroids are at the distance less then \\(\\epsilon\\) from old centroids We use euclidean distance. Prediction for every new observation is simply class of nearest centroid. Algorithm is interpretable, because we can visualise centroids as images. Thanks to using K-means to find subclasses our images are not blurry. Also because number of all subclasses is much lower than number of records in data set using KNN only on centroids is much faster. Consider the following dataset: fig : An example of centroid image In fact, a good subset of the training data set is enough to create a very good classifier. For instance we can choose: fig : An example of centroid image Chosen points approximate sufficiently training data distribution. Notice, that we in fact don’t have to choose particular observations. We can instead chose points in observation space that are similar to observations. This is what k-means algorithm do and so, we can obtain good training data approximation using k-means. Here’s an example of a centroid image: fig : An example of a centroid image 3.8.6.2.2 3.8.5.2.2 SVD SVD is a standard method of dimensionality reduction. It is rewriting \\(m\\times n\\) matrix \\(M\\) as \\(U\\Sigma V^T\\) where \\(U\\) is \\(m\\times m\\) orthonormal matrix, \\(V^T\\) is \\(n\\times n\\) orthonormal matrix and \\(\\Sigma\\) is \\(m\\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal. We assume that singular values of \\(\\Sigma\\) are in descending order. Now by taking first columns of \\(V^T\\) we get vectors which are the most relevent. Let \\(V_n\\) be matrix, whoose columns are \\(V\\) columns with n greatest eigenvalues. Such matrix is linear transformation matrix, that turns observations into their embeddings. It can be shown, that \\(V_n\\) is the best transformation in L2 norm sense. We can then visualise this vectors and see which parts of the picture are the most importent. Also we can reduce number of dimensions for KNN. Most importenet vector Image from Dataset Image after using vector as filter 3.8.6.2.3 3.8.5.2.3 Convolutional Autoencoder We can create semi interpretable model by training a convolutional autoencoder and then creating KNN classifier on pretrained embeddings. As mentioned previously, it has several advantages over KNN, because it uses euclidean distance in more meaningful space. Embedder is not interpretable, but our classifier can at least show us historical observations, that had an impact on prediction, which sometimes is good enough, especially when it can be easily seen why two images are similar and we only want a computer to do humans work. For instance, if we provide 5 images of coala that caused that our image of coala has been interpreted as coala, we maybe don’t know, why those images are similar according to our classifier, however we can see, that they are similar, so further explanation of a model is not required. This model, again, is not fully interpretable. Our implementation of convolutional autoencoder consist of the following layers: Conv2d: Input Channels: 1 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 1 filter size: 5 Conv2d: Input Channels: 1 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 1 filter size: 5 along with pooling and unpooling beetween. fig. : Architecture of the convolutional autoencoder 3.8.7 3.8.6 Black-Box Convolutional Neural Networks Classical approach in computer vision is to use convolutional neural networks. A standard artificial neural network sees all variables as being independent from each other. It doesn’t capture the same patterns across image space, nor it recognises, that two pixels next to each other are somehow related. Shifted image is something completly different to a standard neural network from it’s original. Therefore, training a standard neural network is tricky, it requires very big dataset and takes a lot of time. There is, however, a smarter approach that has a capacity to cope with those problems. Namely, convolutional neural networks. A convolutional neural network is an artificial neural network, that tries to capture spacial dependencies between variables, for instance dimensions of pixels that are close to each other. It does that via introducing convolution. The easiest interpretation of convolutional neural network is that instead of training training big network that uses all variables (in our case all pixels), we train smaller transformation with smaller number of variables (smaller subset of pixels close to each other), that we use in many different places on the image. In some sense we train filters. Every filter produces a corresponding so called “channel”. After first layer we can continue filtering channels using convolutional layers. We place a danse layer (or a number of them) at the end and it’s result is our prediction. For further reading, please see … . Having a very good performance, they are impossible to explain. There are some techniques of visualising filters, however more complex networks are generally uninterpretable. Along with standard artificial neural network, we will use it as an instance of robust classifier for comparing results. Our implementation of convolutional neural networks consist of the following layers: Conv 2d: Input Channels: 1 Output Channels: 50 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 50 Output Channels: 70 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 70 Output Channels: 100 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 100 Output Channels: 150 filter size: 5 Linear: Input_size: 1350 Output_size: 500 Linear: Input_size: 200 Output_size: 10 Here’s architecture’s visualisation: 3.8.8 Results Model ACC Black-Box Convolutional 0.941 Logistic regression 0.847 KNN base 0.8606 KNN SVD 0.8001 KNN K-means 0.8512 KNN Convolutional 0.902 3.8.9 Conclusions 3.8.10 Bibliography "],
["acknowledgements.html", "Chapter 4 Acknowledgements", " Chapter 4 Acknowledgements This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as cornerstone for this reopsitory. "],
["references-2.html", "References", " References accenture. 2018. “UNDERSTANDING Machines: EXPLAINABLE Ai,” 19. https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g. Alejandro Barredo Arrieta, Javier Del Ser, Natalia Díaz-Rodríguez. 2019. “Explainable Artificial Intelligence (Xai): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible Ai,” 67. https://arxiv.org/abs/1910.10045. Anda, Bente, Dag Sjøberg, and Audris Mockus. 2009. “Variability and Reproducibility in Software Engineering: A Study of Four Companies That Developed the Same System.” Software Engineering, IEEE Transactions on 35 (July): 407–29. https://doi.org/10.1109/TSE.2008.89. Anderson, Christopher, Joanna Anderson, Marcel van Assen, Peter Attridge, Angela Attwood, Jordan Axt, Molly Babel, et al. 2019. “Reproducibility Project: Psychology.” https://doi.org/10.17605/OSF.IO/EZCUJ. Ardia, David, Lennart F. Hoogerheide, and Herman K. van Dijk. 2009. “AdMit.” The R Journal 1 (1): 25–30. https://doi.org/10.32614/RJ-2009-003. Baldominos, Alejandro, Iván Blanco, Antonio Moreno, Rubén Iturrarte, Óscar Bernárdez, and Carlos Afonso. 2018. “Identifying Real Estate Opportunities Using Machine Learning.” Applied Sciences 8 (November): 2321. https://doi.org/10.3390/app8112321. Batista, Gustavo E. A. P. A., and Maria Carolina Monard. 2003. “An Analysis of Four Missing Data Treatment Methods for Supervised Learning.” Applied Artificial Intelligence 17 (5-6): 519–33. https://doi.org/10.1080/713827181. Bernd Bischl, Jakob Bossek, Jakob Richter. 2018. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions,” 23. https://arxiv.org/abs/1703.03373. Biecek, Przemyslaw. 2018. “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research 19 (84): 1–5. http://jmlr.org/papers/v19/18-416.html. Biecek, Przemyslaw, and Marcin Kosinski. 2017. “archivist: An R Package for Managing, Recording and Restoring Data Analysis Results.” Journal of Statistical Software 82 (11): 1–28. https://doi.org/10.18637/jss.v082.i11. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016a. “Mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. ———. 2016b. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. Bischl, Bernd, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas, and Michel Lang. 2017. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions.” arXiv Preprint arXiv:1703.03373. Bono, Christine, L. Ried, Carole Kimberlin, and Bruce Vogel. 2007. “Missing Data on the Center for Epidemiologic Studies Depression Scale: A Comparison of 4 Imputation Techniques.” Research in Social &amp; Administrative Pharmacy : RSAP 3 (April): 1–27. https://doi.org/10.1016/j.sapharm.2006.04.001. Boughorbel S., El-Anbari M., Jarray F. 2017. “Optimal Classifier for Imbalanced Data Using Matthews Correlation Coefficient Metric.” PLOS One. https://doi.org/https://doi.org/10.1371/journal.pone.0177678. Broatch, Jennifer, Jennifer Green, and Andrew Karl. 2018. “RealVAMS: An R Package for Fitting a Multivariate Value- added Model (VAM).” The R Journal 10 (1): 22–30. https://doi.org/10.32614/RJ-2018-033. Brown, Patrick, and Lutong Zhou. 2010. “MCMC for Generalized Linear Mixed Models with glmmBUGS.” The R Journal 2 (1): 13–17. https://doi.org/10.32614/RJ-2010-003. Buuren, Stef van, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://www.jstatsoft.org/article/view/v045i03. Casalicchio, Giuseppe, Bernd Bischl, Dominik Kirchhoff, Michel Lang, Benjamin Hofner, Jakob Bossek, Pascal Kerschke, and Joaquin Vanschoren. 2019. OpenML: Open Machine Learning and Open Data Platform. https://CRAN.R-project.org/package=OpenML. “Census dataset.” 1996. http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html. Coeurjolly, J.-F., R. Drouilhet, P. Lafaye de Micheaux, and J.-F. Robineau. 2009. “asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples.” The R Journal 1 (2): 26–30. https://doi.org/10.32614/RJ-2009-015. Computing Machinery, Association for. 2018. “Artifact Review and Badging.” https://www.acm.org/publications/policies/artifact-review-badging. Conway, Jennifer. 2018. “Artificial Intelligence and Machine Learning : Current Applications in Real Estate.” PhD thesis. https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf. Din, Allan, Martin Hoesli, and André Bender. 2001. “Environmental Variables and Real Estate Prices.” Urban Studies 38 (February). https://doi.org/10.1080/00420980120080899. Dray, Stéphane, and Anne-Béatrice Dufour. 2007. “The Ade4 Package: Implementing the Duality Diagram for Ecologists.” Journal of Statistical Software, Articles 22 (4): 1–20. https://doi.org/10.18637/jss.v022.i04. Drummond, Chris. 2012. “Reproducible Research: A Dissenting Opinion.” In. Eisner, D. A. 2018. “Reproducibility of Science: Fraud, Impact Factors and Carelessness.” Journal of Molecular and Cellular Cardiology 114 (January): 364–68. https://doi.org/10.1016/j.yjmcc.2017.10.009. Elmenreich, Wilfried, Philipp Moll, Sebastian Theuermann, and Mathias Lux. 2018. “Making Computer Science Results Reproducible - a Case Study Using Gradle and Docker,” August. https://doi.org/10.7287/peerj.preprints.27082v1. Fan, Gang-Zhi, Seow Eng Ong, and Hian Koh. 2006. “Determinants of House Price: A Decision Tree Approach.” Urban Studies 43 (November): 2301–16. https://doi.org/10.1080/00420980600990928. Fenton, N. E., and S. L. Pfleeger. 1997. Software Metrics: A Rigorous &amp; Practical Approach. International Thompson Press. Fern’andez, Daniel M’endez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” CoRR abs/1904.06499. http://arxiv.org/abs/1904.06499. Fernández, Daniel Méndez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” ArXiv abs/1904.06499. Flach, Peter, Jose Hernandez-Orallo, and Cèsar Ferri. 2011. “A Coherent Interpretation of Auc as a Measure of Aggregated Classification Performance.” In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 657–64. Flach Peter, Ferri, Hernandez-Orallo Jose. 2011. “A Coherent Interpretation of Auc as a Measure of Aggregated Classification Performance.” Proceedings of the 28th International Conference on Machine Learning. https://icml.cc/2011/papers/385_icmlpaper.pdf. Fomel, Sergey, Paul Sava, Ioan Vlad, Yang Liu, and Vladimir Bashkardin. 2013. “Madagascar: Open-Source Software Project for Multidimensional Data Analysis and Reproducible Computational Experiments.” Journal of Open Research Software 1 (November): e8. https://doi.org/10.5334/jors.ag. Friedman, Jerome. 2000. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (November). https://doi.org/10.1214/aos/1013203451. Gao, Guangliang, Zhifeng Bao, Jie Cao, A. Qin, Timos Sellis, Fellow, IEEE, and Zhiang Wu. 2019. “Location-Centered House Price Prediction: A Multi-Task Learning Approach,” January. https://arxiv.org/abs/1901.01774. Garrod, Guy D, and Kenneth G Willis. 1992. “Valuing Goods’ Characteristics: An Application of the Hedonic Price Method to Environmental Attributes.” Journal of Environmental Management 34 (1): 59–76. Gentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23. Goodman, Steven N., Daniele Fanelli, and John P. A. Ioannidis. 2016a. “What Does Research Reproducibility Mean?” Science Translational Medicine 8 (341). https://doi.org/10.1126/scitranslmed.aaf5027. ———. 2016b. “What Does Research Reproducibility Mean?” Science Translational Medicine 8 (341): 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027. Gosiewska, Alicja, and Przemyslaw Biecek. 2020. “Lifting Interpretability-Performance Trade-off via Automated Feature Engineering.” https://arxiv.org/abs/2002.04267. Gosiewska, Alicja, Aleksandra Gacek, Piotr Lubon, and Przemyslaw Biecek. 2019. “SAFE Ml: Surrogate Assisted Feature Extraction for Model Learning.” http://arxiv.org/abs/1902.11035. Guazzelli, Alex, Michael Zeller, Wen-Ching Lin, and Graham Williams. 2009. “PMML: An Open Standard for Sharing Models.” The R Journal 1 (1): 60–65. https://doi.org/10.32614/RJ-2009-010. Gundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence.” https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248. Günther, Frauke, and Stefan Fritsch. 2010. “neuralnet: Training of Neural Networks.” The R Journal 2 (1): 30–38. https://doi.org/10.32614/RJ-2010-006. Halstead, M. H. 1977. Elements of Software Science. Elsevier. Hankin, Robin. 2007. “Introducing Untb, an R Package for Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity.” Journal of Statistical Software, Articles 22 (12): 1–15. https://doi.org/10.18637/jss.v022.i12. Hastie, Trevor, and Rahul Mazumder. 2015a. “SoftImpute: Matrix Completion via Iterative Soft-Thresholded Svd.” https://CRAN.R-project.org/package=softImpute. ———. 2015b. SoftImpute: Matrix Completion via Iterative Soft-Thresholded Svd. https://CRAN.R-project.org/package=softImpute. Heyman, Axel, and Dag Sommervoll. 2019. “House Prices and Relative Location.” Cities 95 (September): 102373. https://doi.org/10.1016/j.cities.2019.06.004. Hu, Feng, and Hang Li. 2013. “A Novel Boundary Oversampling Algorithm Based on Neighborhood Rough Set Model: NRSBoundary-Smote.” Mathematical Problems in Engineering 2013 (November). https://doi.org/10.1155/2013/694809. Hung, Ling-Hong, Daniel Kristiyanto, Sung Lee, and Ka Yee Yeung. 2016. “GUIdock: Using Docker Containers with a Common Graphics User Interface to Address the Reproducibility of Research.” PloS One 11 (April): e0152686. https://doi.org/10.1371/journal.pone.0152686. Josse, Julie, and François Husson. 2016. “missMDA: A Package for Handling Missing Values in Multivariate Data Analysis.” Journal of Statistical Software 70 (1): 1–31. https://doi.org/10.18637/jss.v070.i01. “Journal Metrics - Impact, Speed and Reach.” n.d. https://www.journals.elsevier.com/international-journal-of-approximate-reasoning/news/journal-metricsimpact-speed-and-reach. Kim, Donghoh, and Hee-Seok Oh. 2009. “EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum.” The R Journal 1 (1): 40–46. https://doi.org/10.32614/RJ-2009-002. Kitzes, Justin, Daniel Turek, and Fatma Deniz. 2017. The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. Univ of California Press. Kowarik, Alexander, and Matthias Templ. 2016a. “Imputation with the R Package Vim.” Journal of Statistical Software 74 (7): 1–16. https://www.jstatsoft.org/article/view/v045i03. ———. 2016b. “Imputation with the R Package VIM.” Journal of Statistical Software 74 (7): 1–16. https://doi.org/10.18637/jss.v074.i07. Landau, William Michael. 2018. “The Drake R Package: A Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 3 (21). https://doi.org/10.21105/joss.00550. Law, Stephen. 2017. “Defining Street-Based Local Area and Measuring Its Effect on House Price Using a Hedonic Price Approach: The Case Study of Metropolitan London.” Cities 60 (February): 166–79. https://doi.org/10.1016/j.cities.2016.08.008. LeVeque, Randall. 2009. “Python Tools for Reproducible Research on Hyperbolic Problems.” Computing in Science &amp; Engineering 11 (January): 19–27. https://doi.org/10.1109/MCSE.2009.13. Liu, Rui, and Lu Liu. 2019. “Predicting housing price in China based on long short-term memory incorporating modified genetic algorithm.” Soft Computing, 1–10. https://doi.org/10.1007/s00500-018-03739-w. Marwick, B. n.d. “Rrtools: Creates a Reproducible Research Compendium (2018).” Marwick, Ben. 2016. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation.” Journal of Archaeological Method and Theory 24 (2): 424–50. https://doi.org/10.1007/s10816-015-9272-9. Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2017. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986. Mayer, Michael. 2019. “MissRanger: Fast Imputation of Missing Values.” https://CRAN.R-project.org/package=missRanger. McCabe, T. J. 1976. “A Complexity Measure.” IEEE Transactions on Software Engineering 2 (4): 308–20. McNutt, Marcia. 2014. “Journals Unite for Reproducibility.” Science 346 (6210): 679–79. https://doi.org/10.1126/science.aaa1724. Melo], Vinícius [Veloso de, and Wolfgang Banzhaf. 2018. “Automatic Feature Engineering for Regression Models with Machine Learning: An Evolutionary Computation and Statistics Hybrid.” Information Sciences 430-431: 287–313. https://doi.org/https://doi.org/10.1016/j.ins.2017.11.041. Mevik, Björn-Helge, and Ron Wehrens. 2007. “The Pls Package: Principal Component and Partial Least Squares Regression in R.” Journal of Statistical Software, Articles 18 (2): 1–23. https://doi.org/10.18637/jss.v018.i02. Mi, Xuefei, Tetsuhisa Miwa, and Torsten Hothorn. 2009. “New Numerical Algorithm for Multivariate Normal Probabilities in Package mvtnorm.” The R Journal 1 (1): 37–39. https://doi.org/10.32614/RJ-2009-001. Molnar, Christoph. 2019. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/simple.html. Murdoch, Kumbier, Singh. 2018. “Interpretable Machine Learning: Definitions, Methods, and Applications,” 2. https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ. Musil, Carol, Camille Warner, Piyanee Klainin-Yobas, and Susan Jones. 2002. “A Comparison of Imputation Techniques for Handling Missing Data.” Western Journal of Nursing Research 24 (December): 815–29. https://doi.org/10.1177/019394502762477004. Özalp, Ayşe, and Halil Akinci. 2017. “The Use of Hedonic Pricing Method to Determine the Parameters Affecting Residential Real Estate Prices.” Arabian Journal of Geosciences 10 (December). https://doi.org/10.1007/s12517-017-3331-3. Pandey. 2019. “Interpretable Machine Learning: Extracting Human Understandable Insights from Any Machine Learning Model,” April. https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b. Park, Byeonghwa, and Jae Bae. 2015. “Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data.” Expert Systems with Applications 42 (April). https://doi.org/10.1016/j.eswa.2014.11.040. Patil, Prasad, Roger D. Peng, and Jeffrey T. Leek. 2016. “A Statistical Definition for Reproducibility and Replicability.” Science. https://doi.org/10.1101/066803. Patricia Arroba, Marina Zapater, José L. Risco-Martín. 2015. “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering.” Journal of Grid Computing 13: 409–23. https://doi.org/10.1007/s10723-014-9313-8. Peng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–7. https://doi.org/10.1126/science.1213847. Piccolo, Stephen R., and Michael B. Frampton. 2016. “Tools and Techniques for Computational Reproducibility.” GigaScience 5 (1). https://doi.org/10.1186/s13742-016-0135-4. Raff, Edward. 2020. “Quantifying Independently Reproducible Machine Learning.” https://thegradient.pub/independently-reproducible-machine-learning/. Rafiei, Mohammad H., and Hojjat Adeli. 2015. “A Novel Machine Learning Model for Estimation of Sale Prices of Real Estate Units.” Journal of Construction Engineering and Management 142 (August). https://doi.org/10.1061/(ASCE)CO.1943-7862.0001047. Randeniya, TD, Gayani Ranasinghe, and Susantha Amarawickrama. 2017. “A Model to Estimate the Implicit Values of Housing Attributes by Applying the Hedonic Pricing Method.” International Journal of Built Environment and Sustainability 4 (May). https://doi.org/10.11113/ijbes.v4.n2.182. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. “Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing.” 2014. http://ropensci.github.io/reproducibility-guide/. Rosenberg, David E., Yves Filion, Rebecca Teasley, Samuel Sandoval-Solis, Jory S. Hecht, Jakobus E. van Zyl, George F. McMahon, Jeffery S. Horsburgh, Joseph R. Kasprzyk, and David G. Tarboton. 2020. “The Next Frontier: Making Research More Reproducible.” Journal of Water Resources Planning and Management 146 (6): 01820002. https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215. Rubin, DONALD B. 1976. “Inference and missing data.” Biometrika 63 (3): 581–92. https://doi.org/10.1093/biomet/63.3.581. Sakia, R. M. 1992. “The Box-Cox Transformation Technique: A Review.” Journal of the Royal Statistical Society. Series D (the Statistician) 41 (2): 169–78. http://www.jstor.org/stable/2348250. Sayyad Shirabad, J., and T. J. Menzies. 2005. “The PROMISE Repository of Software Engineering Databases.” School of Information Technology and Engineering, University of Ottawa, Canada. http://promise.site.uottawa.ca/SERepository. Selim, Hasan. 2009. “Determinants of House Prices in Turkey: Hedonic Regression Versus Artificial Neural Network.” Expert Syst. Appl. 36 (March): 2843–52. https://doi.org/10.1016/j.eswa.2008.01.044. Slezak, Peter, and Iveta Waczulikova. 2011. “Reproducibility and Repeatability.” Physiological Research / Academia Scientiarum Bohemoslovaca 60 (April): 203–4; author reply 204. Soetaert, Karline, Thomas Petzoldt, and R. Woodrow Setzer. 2010. “Solving Differential Equations in R.” The R Journal 2 (2): 5–15. https://doi.org/10.32614/RJ-2010-013. Stanisic, Luka, Arnaud Legrand, and Vincent Danjean. 2015. “An Effective Git and Org-Mode Based Workflow for Reproducible Research.” SIGOPS Oper. Syst. Rev. 49 (1): 61–70. https://doi.org/10.1145/2723872.2723881. Stef van Buuren, Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in R.” https://www.jstatsoft.org/article/view/v045i03. Stekhoven, Daniel J., and Peter Buehlmann. 2012. “MissForest - Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. Stodden, Victoria, David H. Bailey, Jonathan M. Borwein, Randall J. LeVeque, William J. Rider, and William Stein. 2013. “Setting the Default to Reproducible Reproducibility in Computational and Experimental Mathematics.” In. Stodden, Victoria, Marcia McNutt, David H. Bailey, Ewa Deelman, Yolanda Gil, Brooks Hanson, Michael A. Heroux, John P. A. Ioannidis, and Michela Taufer. 2016. “Enhancing Reproducibility for Computational Methods.” Science 354 (6317): 1240–1. https://doi.org/10.1126/science.aah6168. Stodden, Victoria, Jennifer Seiler, and Zhaokun Ma. 2018. “An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility.” Proceedings of the National Academy of Sciences 115 (11): 2584–9. https://doi.org/10.1073/pnas.1708290115. Strobl, Carolin, Torsten Hothorn, and Achim Zeileis. 2009. “Party on!” The R Journal 1 (2): 14–17. https://doi.org/10.32614/RJ-2009-013. Su, Xiaoyuan, Taghi Khoshgoftaar, and Russ Greiner. 2008. “Using Imputation Techniques to Help Learn Accurate Classifiers.” Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI 1 (December): 437–44. https://doi.org/10.1109/ICTAI.2008.60. Therneau, Terry, and Beth Atkinson. 2019. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. Thomas, Kluyver, Ragan-Kelley Benjamin, P&amp;eacute;rez Fernando, Granger Brian, Bussonnier Matthias, Frederic Jonathan, Kelley Kyle, et al. 2016. “Jupyter Notebooks &amp;Ndash; a Publishing Format for Reproducible Computational Workflows.” Stand Alone 0 (Positioning and Power in Academic Publishing: Players, Agents and Agendas): 87–90. https://doi.org/10.3233/978-1-61499-649-1-87. van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://www.jstatsoft.org/v45/i03/. Vandewalle, Patrick, Jelena Kovacevic, and Martin Vetterli. 2009. “Reproducible Research in Signal Processing.” IEEE Signal Processing Magazine 26 (3): 37–47. Vanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. “OpenML: Networked Science in Machine Learning.” SIGKDD Explorations 15 (2): 49–60. https://doi.org/10.1145/2641190.2641198. Velez, White, D. R., and J. H Moore. 2007. “A Balanced Accuracy Function for Epistasis Modeling in Imbalanceddatasets Using Multifactor Dimensionality Reduction.” Genetic Epidemiology, no. 31: 306–15. https://doi.org/10.1002/gepi.20211. Wilhelm, Stefan, and B. G. Manjunath. 2010. “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution.” The R Journal 2 (1): 25–29. https://doi.org/10.32614/RJ-2010-005. Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01. Yuan, Lester. 2007. “Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition: The R Package Bio.infer.” Journal of Statistical Software, Articles 22 (3): 1–20. https://doi.org/10.18637/jss.v022.i03. "]
]
